---
layout: post
title: "NLP 심화발제"
date: 2024-11-06
categories: NLP
---
# Full Fine-Tuning
* Fine-tuning 중 하나로 사전 학습된 모델을 특정 작업에 맞게 재학습하여 최적화하는 과정
* 모델의  **모든 가중치**를 조정하는 방식
* 단점 : 메모리와 계산 비용이 매우 많이 필요

# Parameter-Efficient Fine-Tuning(PET)
* 위의 비용문제를 해결하는 방식
* 전체 모델의 **일부 파라미터만** 조정하거나, 추가적인 모듈을 추가하여 효율적으로 fine-tuning 하는 방식   

## LoRA
* PET 중 하나인 Low-Rank Adaptation
* 일부 파라미터만 효율적으로 조정
* 저차원 행렬을 추가해 모델을 조정함으로써 메모리와 연산 자원을 절약

### LoRA 구조
![개념](/img/lora1.png)         
#### 파란색 : Pretrained Weights (사전 학습된 가중치) 
* 이 가중치는 모델이 처음에 학습된 상태로, 학습 중에 **Freeze**되어 업데이트되지 않는 부분

#### 주황색 : 저차원 행렬 A,B
* 학습 시 업데이트되는 부분
* 행렬 B는 초기화 시에 값이 0으로 설정, A는 평균이 0이고 분산이 σ^2인 정규분포로 초기화
* A,B의 차원은 r로 낮음

#### foward pass
* 입력 x(1×d행렬)가 모델로 들어오면, 기존의 사전 학습된 가중치 W(d×d행렬)와 곱해짐 -> 모델의 기본예측(1×d행렬)
* ΔW = B×A, ΔWx를 계산
* 최종출력 h = Wx + (B×A)x (1×d 행렬)를 만듦


```python
import torch
import torch.nn as nn

class LoRA(nn.Module):
    def __init__(self, input_dim, output_dim, rank):
        super(LoRA, self).__init__()
        
        # 기존의 사전 학습된 가중치 W
        self.W = nn.Linear(input_dim, output_dim, bias=False)
        
        # W의 파라미터를 Freeze 
        for param in self.W.parameters():
            param.requires_grad = False  # W를 Freeze하여 학습하지 않도록 설정
        
        # 추가된 저차원 행렬 A와 B
        self.A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)  # 작은 값으로 초기화
        self.B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)
        
    def forward(self, x):
        # 기본 가중치 W와 입력 x의 곱
        Wx = self.W(x)
        
        # 저차원 행렬 A와 B의 곱을 추가하여 ΔW를 계산
        delta_Wx = x @ self.A @ self.B
        
        # 최종 출력: Wx + ΔWx 
        return Wx + delta_Wx

# 예시: 입력과 출력의 차원을 지정하여 모델 만드는 과정
input_dim = 768  # 예: 입력 차원
output_dim = 768  # 예: 출력 차원
rank = 4  # 저차원 크기

model = LoRA(input_dim, output_dim, rank)
```

#### 가중치 행렬을 Freeze하는 방식
* 딥러닝 프레임워크(예: PyTorch, TensorFlow)에서는 가중치를 Freeze할 수 있는 옵션존재
* "requires_grad" 사용해 해당 가중치가 학습 중에 업데이트되지 않도록 함

#### 저차원 행렬 A와 B의 결정 방식
* B의 차원을 r×d, A의 차원을 d×r
* 차원 r이 낮아지면 계산 비용이 줄어들지만, 너무 작으면 성능이 떨어짐
* 논문에서는 실험적으로 r 값을 설정함

#### 차원이 줄어드는 정도 
d=4096, r=16 으로 가정시     
W 파라미터 수 : 4096 x 4096 = 16,777,216 개    
A, B 파라미터 수의 합 : 4096x16(ㅁ) + 16x4096 = 65,536 + 65,536 = 131,072
### LoRA의 Transformer 적용 



