---
layout: post
title: "NLP 심화발제"
date: 2024-11-06
categories: NLP
---
# Full Fine-Tuning
* Fine-tuning 중 하나로 사전 학습된 모델을 특정 작업에 맞게 재학습하여 최적화하는 과정
* 모델의 **모든 가중치**를 조정하는 방식
* 단점 : 메모리와 계산 비용이 매우 많이 필요

# Parameter-Efficient Fine-Tuning(PET)
* 위의 비용문제를 해결하는 방식
* 전체 모델의 **일부 파라미터만** 조정하거나, 추가적인 모듈을 추가하여 효율적으로 fine-tuning 하는 방식   

## Adapter Tuning 
* adapter는 대형 모델의 각 레이어에 추가되는 작은 모듈
* 기존 모델 가중치는 그대로 유지하면서 **Adapter 모듈의 파라미터(추가파라미터)만** 학습하는 방식   
* **저차원**에서 필요한 정보만 학습

![구조](/img/adapter구조.png)      
* 오른쪽:adapter, 왼쪽: adapter를 transformer에 적용 
* Transformer 레이어의 Feed-forward 층이나 레이어 정규화(Layer Norm) 직후에 삽입    

### 구조 
1. Feedforward Up-Projection   
* 입력을 저차원으로 축소하는 선형 변환 ->  파라미터 수를 줄임
2. Nonlinearity
* 일반적으로 ReLU와 같은 비선형 활성화 함수가 사용됌
* 비선형성을 추가 -> 복잡한 패턴을 학습
3. Feedforward Down-Projection  
* 축소된 차원을 다시 원래 차원으로 확장하는 선형 변환
* Adapter는 필요한 정보를 원래 차원으로 되돌려 Transformer 레이어에 전달
* 최종 출력 차원은 원래 차원과 동일하게 유지   

#### Adapter 모듈 요소 
1. Bottle-neck 구조
* 목적 : 파라미터 수를 줄이기 위해 
* 작동 방식 :
    * 첫번째 투영(projection): 입력 차원 d -> m, 파리미터수는 dm개 
    * 두번째 투영(projection): m -> 원래 차원 d, 파라미터수는 md개 
             
* 각 레이어에 추가되는 총 파라미터 수 = 2md+d+m
* 일반적으로 m은 전체 모델 차원의 0.5%~8% 로 설정 
![그림](/img/adapterbottle.png)    
2. Skip-Connection
* 목적 : Adapter 이 차원 축소와 확장을 하더라도 원래 입력정보가 손실없게 유지하기 위해 
* 역할 : 원래 입력을 그대로 다음 레이어로 전달 -> 추가적인 정보 학습이 모델에 통합되도록 연결              
![그림](/img/adapterskio.png)

### 성능    
논문에서 다양한 방법으로 성능을 평가한 것 중 일부   
![성능](/img/adapter성능평가.png)     
* x축 : 값이 클수록 모델에서 학습 가능한 파라미터가 많아짐
* y축 : 성능 변화, 0은 기본 모델 성능과 동일한 경우 
* Adapter Tuning: 0에 가까운 위치에서 안정적으로 유지 -> 적은 수의 학습 가능한 파라미터로 성능을 유지
* Fine-tune top layers : 파라미터가 적을 때 성능이 크게 떨어짐 -> 파라미터 수가 증가하면서 성능이 개선     


## LoRA
* PET 중 하나인 Low-Rank Adaptation
* 일부 파라미터만 효율적으로 조정
* 저차원 행렬을 추가해 모델을 조정함으로써 메모리와 연산 자원을 절약

### LoRA 구조
![개념](/img/lora1.png)         
1. 파란색 : Pretrained Weights (사전 학습된 가중치)
* 이 가중치는 모델이 처음에 학습된 상태로, 학습 중에 **Freeze**되어 업데이트되지 않는 부분

2. 주황색 : 저차원 행렬 A,B
* 학습 시 업데이트되는 부분
* 행렬 B는 초기화 시에 값이 0으로 설정, A는 평균이 0이고 분산이 σ^2인 정규분포로 초기화
* A,B의 차원은 r로 낮음

3. foward pass
* 입력 x(1×d행렬)가 모델로 들어오면, 기존의 사전 학습된 가중치 W(d×d행렬)와 곱해짐 -> 모델의 기본예측(1×d행렬)
* ΔW = B×A, ΔWx를 계산
* 최종출력 h = Wx + (B×A)x (1×d 행렬)를 만듦

![구조](/img/lora비교.png)

#### 가중치 행렬을 Freeze하는 방식
* 딥러닝 프레임워크(예: PyTorch, TensorFlow)에서는 가중치를 Freeze할 수 있는 옵션존재
* "requires_grad" 사용해 해당 가중치가 학습 중에 업데이트되지 않도록 함

#### 저차원 행렬 A와 B의 결정 방식
* B의 차원을 r×d, A의 차원을 d×r
* 차원 r이 낮아지면 계산 비용이 줄어들지만, 너무 작으면 성능이 떨어짐
* 논문에서는 실험적으로 r 값을 설정함

#### 차원이 줄어드는 정도 
d=4096, r=16 으로 가정시     
W 파라미터 수 : 4096 x 4096 = 16,777,216 개    
A, B 파라미터 수의 합 : 4096x16(A) + 16x4096(B) = 65,536 + 65,536 = 131,072 개
-> 파라미터 수가 약 0.78%로 줄어든다.
-> 모든 Layer 에 A,B 를 추가하는 것이 아니라서 실제적으로는 더 줄어든다. 

### 정확도 
![정확도](/img/lora2.png)
논문에 나온 여러 정확도 사진 중 하나 
파라미터 수는 LORA 가 적은데 정확도는 그 이상인것을 확인 할 수 있음       

### LoRA의 Transformer 적용
Transformer의 어떤 가중치 행렬에 LoRA 를 적용해야 성능이 높은지 실험한 결과     
![결과](/img/loraparameters.png)      
* GPT-3 175B 모델의 Self-Attention 모듈 내 가중치들에 테스트 한 결과 
* Wq와 𝑊𝑣를 함께 조정하는 것이 가장 높은 성능을 보여줌   

### Optimal Rank r     
최적의 차원 r 찾는 실험한 결과 
![결과](/img/lorar.png)         
* Wq,Wk,𝑊𝑣,Wo에 r 값을 1, 2, 4, 8, 64로 설정해서 LoRA를 적용 
* r=4에서 가장 높은 성능     

#### subspace similarity between different r
-> 낮은 랭크와 높은 랭크에서 학습된 저차원 공간(subspace)이 얼마나 비슷한지를 평가
-> 낮은 r에서도 충분한 정보를 학습할 수 있는지를 판단
-> 랭크 r=8에서 학습된 하위 공간과 r=64에서 학습된 하위 공간이 높은 유사도를 보인다면, 랭크가 높아져도 학습된 정보의 차이가 크지 않음       
![유사도공식](/img/lora유사도.png)        
![히트맵](/img/loraheatmap.png)     
정규화된 하위 공간 유사도(subspace similarity)는 ϕ(Ar=8,Ar=64,i,j)로 정의되고 값이 1에 가까울 수록 하위공간이 높은 유사도를 가짐
* 밝을 수록 유사도가 높음
* i와 j의 값이 작은 부분, 상위 특이 벡터(중요정보담고있는 벡터)간의 유사도가 높음
* i와 j의 값이 큰 부분, 하위 특이 벡터(노이즈,불필요한정보있는 벡터) 간의 유사도 낮음     

-> 낮은 랭크 r=8으로도 충분한 정보 유지       

### 코드 
```python
import torch
import torch.nn as nn

class LoRA(nn.Module):
    def __init__(self, input_dim, output_dim, rank):
        super(LoRA, self).__init__()
        
        # 기존의 사전 학습된 가중치 W
        self.W = nn.Linear(input_dim, output_dim, bias=False)
        
        # W의 파라미터를 Freeze 
        for param in self.W.parameters():
            param.requires_grad = False  # W를 Freeze하여 학습하지 않도록 설정
        
        # 추가된 저차원 행렬 A와 B
        self.A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)  # 작은 값으로 초기화
        self.B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)
        
    def forward(self, x):
        # 기본 가중치 W와 입력 x의 곱
        Wx = self.W(x)
        
        # 저차원 행렬 A와 B의 곱을 추가하여 ΔW를 계산
        delta_Wx = x @ self.A @ self.B
        
        # 최종 출력: Wx + ΔWx 
        return Wx + delta_Wx

# 예시: 입력과 출력의 차원을 지정하여 모델 만드는 과정
input_dim = 768  # 예: 입력 차원
output_dim = 768  # 예: 출력 차원
rank = 4  # 저차원 크기

model = LoRA(input_dim, output_dim, rank)
```
## QLoRA
* Quantized LoRA로 LoRA의 효율성을 더욱 높이기 위해 **양자화(Quantization)**를 적용한 방식   
* 양자화 :  모델의 가중치나 활성화 값을 저장할 때 사용하는 비트 수를 줄여 표현하는 방법      
-> 숫자의 표현 정밀도를 낮추어 메모리 사용량을 줄이고 연산을 효율화   
-> 32비트 부동 소수점(floating point)에서  8비트 정수(integer) 또는 4비트 정수로 표현      

### 구조
![그림](/img/qlora1.png)        
* 4-bit NormalFloat Quantization
    * NF4는 부동소수점 수를 기반으로 한 4비트 표현
    * 정규 분포(정규화된 분포)를 따르는 데이터에서 최적
    -> 분포의 중심과 밀집된 값을 더 잘 표현할
    * Base Model이 4-bit Transformer로 표시된 부분
    * 수식
     ![수식](/img/qlora양자화1.png)    
        * qi: 양자화된 값
        * Qx: 양자화 함수, 입력된 값을 정해진 양자화 범위 내에서 가장 가까운 값으로 매핑
        * i,k: 인덱스 값과 스케일링 파라미터
* Double Quantization
    * 모든 가중치가 양자화된 후 저장된 양자화 상수(일반적으로 스케일링 팩터나 오프셋)를 다시 양자화
    * 이중 양자화를 통해 필요한 메모리를 크게 줄임
* Paged Optimizers
    * 미니 배치 기반 학습 시 발생할 수 있는 메모리 스파이크 문제를 효과적으로 관리 
    (미니 배치 크기가 커질수록 GPU 메모리가 빠르게 참)
    * NVIDIA의 통합 메모리를 사용하여, 자주 사용되지 않는 데이터를 자동으로 GPU에서 CPU 메모리로 이동시키고, 필요 시 다시 GPU로 이동
    * Paging Flow(분홍색 화살표)

### 수식
* QLoRA에서 단일 선형 레이어의 출력을 계산
![수식](/img/qlora수식1.png)     
    * Xbf16: BF16 형식으로 표현된 입력 데이터
    * doubleDequant: 이중 양자화된 가중치를 BF16 형식으로 복원 
        * 입력값은 이중양자화에서 사용된 매개변수와 가중치 
    * L:BF16 형식으로 표현된 LoRA 어댑터 행렬              
-> ​이중 양자화된 가중치를 복원하고, 입력값과 LoRA 어댑터를 적용하여 최종 출력을 계산
 
* 비양자화(dequantization)
![수식](/img/qlora수식2.png)      
  * 원래 형식으로 복원(dequantize)해야 하므로, 저 수식에서는 두 번의 비양자화 과정을 수행
  * 첫번째 비양자화 : 4비트 양자화된 가중치를 풀어내기 위한 스케일링 값을 계산      
  * 두번째 비양자화 : 첫 번째 비양자화 결과를 이용해 최종적으로 BF16 형식으로 복원        

### 성능
<img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/qlora성능1.png?raw=true" alt="성능" width="280" height="300">
 
* QLoRA-All, QLoRA-FFN, QLoRA-Attention : 모든 레이어, FFN 레이어, 어텐션 레이어에 QLoRA를 적용한 결과
* **Alpaca (ours)** :  QLoRA를 사용하여 Alpaca 데이터셋으로 미세 조정한 모델
* Stanford-Alpaca: 16비트로 미세 조정된 기존 Stanford 모델
* QLoRA-All, QLoRA-FFN, QLoRA-Attention 모두 16비트의 Stanford-Alpaca와 유사하거나 더 높은 성능을 보임

![성능](/img/qlora성능2.png)    
* NF4 + DQ가 높은 메모리 효율성과 함께 다른 양자화 방법과 비슷하거나 그 이상의 성능 보임

### 코드 
```python
pip install transformers
pip install accelerate
pip install bitsandbytes

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from transformers import BitsAndBytesConfig
from datasets import load_dataset

# 1. 모델 및 토크나이저 불러오기
model_name = "facebook/opt-6.7b"  
# 학습할 모델 이름
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. 4비트 양자화 구성 설정
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4비트 양자화 사용
    bnb_4bit_use_double_quant=True,  # 이중 양자화 적용 (더 나은 성능)
    bnb_4bit_quant_type="nf4",       # 양자화 유형 선택 (일반적으로 `nf4`가 좋음)
    bnb_4bit_compute_dtype=torch.float16  # 계산시 사용되는 데이터 유형
)

# 3. QLoRA를 위한 모델 로드
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,  # 양자화 설정 적용
    device_map="auto"  # GPU 자원 자동으로 할당
)

# 4. 데이터셋 준비
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 5. 학습 설정
training_args = TrainingArguments(
    output_dir="./qlora_model",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    warmup_steps=100,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,  # 16-bit 부동 소수점 사용
    logging_steps=10,
)

# 6. Trainer 초기화
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# 7. 학습 시작
trainer.train()
```


### Reference
[Adapter paper](https://arxiv.org/pdf/1902.00751)    
[LoRA paper](https://arxiv.org/abs/2106.09685)     
[QLoRA paper](https://arxiv.org/abs/2305.14314)         
