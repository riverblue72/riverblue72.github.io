---
layout: post
title: "NLP 심화발제"
date: 2024-11-06
categories: NLP
---
# Full Fine-Tuning
* Fine-tuning 중 하나로 사전 학습된 모델을 특정 작업에 맞게 재학습하여 최적화하는 과정
* 모델의  **모든 가중치**를 조정하는 방식
* 단점 : 메모리와 계산 비용이 매우 많이 필요

# Parameter-Efficient Fine-Tuning(PET)
* 위의 비용문제를 해결하는 방식
* 전체 모델의 **일부 파라미터만** 조정하거나, 추가적인 모듈을 추가하여 효율적으로 fine-tuning 하는 방식   

## LoRA
* PET 중 하나인 Low-Rank Adaptation
* 일부 파라미터만 효율적으로 조정
* 저차원 행렬을 추가해 모델을 조정함으로써 메모리와 연산 자원을 절약

### LoRA 구조
![개념](/img/lora1.png)         
#### 파란색 : Pretrained Weights (사전 학습된 가중치) 
* 이 가중치는 모델이 처음에 학습된 상태로, 학습 중에 **Freeze**되어 업데이트되지 않는 부분

#### 주황색 : 저차원 행렬 A,B
* 학습 시 업데이트되는 부분
* 행렬 B는 초기화 시에 값이 0으로 설정, A는 평균이 0이고 분산이 σ^2인 정규분포로 초기화
* A,B의 차원은 r로 낮음

#### foward pass
* 입력 x(1×d행렬)가 모델로 들어오면, 기존의 사전 학습된 가중치 W(d×d행렬)와 곱해짐 -> 모델의 기본예측(1×d행렬)
* ΔW = B×A, ΔWx를 계산
* 최종출력 h = Wx + (B×A)x (1×d 행렬)를 만듦


```python
import torch
import torch.nn as nn

class LoRA(nn.Module):
    def __init__(self, input_dim, output_dim, rank):
        super(LoRA, self).__init__()
        
        # 기존의 사전 학습된 가중치 W
        self.W = nn.Linear(input_dim, output_dim, bias=False)
        
        # W의 파라미터를 Freeze 
        for param in self.W.parameters():
            param.requires_grad = False  # W를 Freeze하여 학습하지 않도록 설정
        
        # 추가된 저차원 행렬 A와 B
        self.A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)  # 작은 값으로 초기화
        self.B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)
        
    def forward(self, x):
        # 기본 가중치 W와 입력 x의 곱
        Wx = self.W(x)
        
        # 저차원 행렬 A와 B의 곱을 추가하여 ΔW를 계산
        delta_Wx = x @ self.A @ self.B
        
        # 최종 출력: Wx + ΔWx 
        return Wx + delta_Wx

# 예시: 입력과 출력의 차원을 지정하여 모델 만드는 과정
input_dim = 768  # 예: 입력 차원
output_dim = 768  # 예: 출력 차원
rank = 4  # 저차원 크기

model = LoRA(input_dim, output_dim, rank)
```

#### 가중치 행렬을 Freeze하는 방식
* 딥러닝 프레임워크(예: PyTorch, TensorFlow)에서는 가중치를 Freeze할 수 있는 옵션존재
* "requires_grad" 사용해 해당 가중치가 학습 중에 업데이트되지 않도록 함

#### 저차원 행렬 A와 B의 결정 방식
* B의 차원을 r×d, A의 차원을 d×r
* 차원 r이 낮아지면 계산 비용이 줄어들지만, 너무 작으면 성능이 떨어짐
* 논문에서는 실험적으로 r 값을 설정함

#### 차원이 줄어드는 정도 
d=4096, r=16 으로 가정시     
W 파라미터 수 : 4096 x 4096 = 16,777,216 개    
A, B 파라미터 수의 합 : 4096x16(A) + 16x4096(B) = 65,536 + 65,536 = 131,072 개
-> 파라미터 수가 약 0.78%로 줄어든다.
-> 모든 Layer 에 A,B 를 추가하는 것이 아니라서 실제적으로는 더 줄어든다. 

#### 정확도 
[정확도](/img/lora2.png)
논문에 나온 여러 정확도 사진 중 하나 
파라미터 수는 LORA 가 적은데 정확도는 그 이상인것을 확인 할 수 있음       

### LoRA의 Transformer 적용
Transformer의 어떤 가중치 행렬에 LoRA 를 적용해야 성능이 높은지 실험한 결과     
![결과](/img/loraparameters.png)      
* GPT-3 175B 모델의 Self-Attention 모듈 내 가중치들에 테스트 한 결과 
* Wq와 𝑊𝑣를 함께 조정하는 것이 가장 높은 성능을 보여줌   

### Optimal Rank r     
최적의 차원 r 찾는 실험한 결과 
![결과](/img/lorar.png)         
* Wq,Wk,𝑊𝑣,Wo에 r 값을 1, 2, 4, 8, 64로 설정해서 LoRA를 적용 
* r=4에서 가장 높은 성능     

#### subspace similarity between different r
-> 낮은 랭크와 높은 랭크에서 학습된 저차원 공간(subspace)이 얼마나 비슷한지를 평가
-> 낮은 r에서도 충분한 정보를 학습할 수 있는지를 판단
-> 랭크 r=8에서 학습된 하위 공간과 r=64에서 학습된 하위 공간이 높은 유사도를 보인다면, 랭크가 높아져도 학습된 정보의 차이가 크지 않음       
![유사도공식](/img/lora유사도.png)        
![히트맵](/img/loraheatmap.png)     
정규화된 하위 공간 유사도(subspace similarity)는 ϕ(Ar=8,Ar=64,i,j)로 정의되고 값이 1에 가까울 수록 하위공간이 높은 유사도를 가짐
* 밝을 수록 유사도가 높음
* i와 j의 값이 작은 부분, 상위 특이 벡터(중요정보담고있는 벡터)간의 유사도가 높음
* i와 j의 값이 큰 부분, 하위 특이 벡터(노이즈,불필요한정보있는 벡터) 간의 유사도 낮음     

-> 낮은 랭크 r=8으로도 충분한 정보 유지        