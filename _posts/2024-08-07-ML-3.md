---
layout: post
title:  "ML 3주차: 혼공머신 3단원 "
date:   2024-08-07 12:00
categories: ML_Study
---
# 03. 회귀 알고리즘과 모델 규제   
## 01. knn 회귀   
#### 회귀    
지도학습 알고리즘에는 크게 분류와 회귀로 나뉜다. 회귀는 임의의 어떤 숫자를 예측하는 것이다. 또한 두 변수의 상관관계를 분석하는 방법을 회귀라고 한다.      
![회귀](/img/회귀개념.png)    
### 이 단원 목적    
농어의 길이
### 과정   
#### 1. 데이터 준비하기    
#### 농어 길이와 무게 데이터 준비   
``` python  
import numpy as np
perch_length=np.array([농어 길이 데이터])
perch_weight=np.array([농어 무게 데이터])
#산점도 그래프 그려보면 길이가 커지면 무게가 커짐을 확인할 수 있다. 
from sklearn.model_selection import train_test_split  
train_input, test_iput, train_target, test_target = train_test_split(perch_length, perch_weight,random_state=42)
```
#### 2차원배열로 변경하기   
```python
#-1 이라는 숫자는 자동으로 크기 입력하게 해줌   
# 예시: [1,2,3,4] 에서 reshape(2,2):2*2행렬로 변경 reshape(2,-1):2*2 로 계산해줌  
train_input = train_input.reshape(-1,1)  
test_input = test_input.reshape(-1,1)
```   
#### 2. k 최근접 이웃 회귀 알고리즘   
#### 훈련시키기  
``` python  
from sklearn.neighbors import KNeighborsRegressor  
knr = KNeighborsRegressor()
knr.fit(train_input, trian_target)  
```
#### 평가하기, 결정계수 R^2  
```python
print(knr.score(test_input, test_input))   #출력값:0.99   
```
회귀일땐 결정계수로 평가한다. 결정계수 R^2 = 1 - (타깃 - 예측)^2 / (타깃 - 평균)^2      
예측값이 타깃값에 가까울 수록, 즉 정확도가 높을 수록 결정계수는 1에 가까워 진다.      

#### 참고: 예측값의 절댓값 오차 평균구하기   
``` python
from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)    #예측값
mae = mean_absolute_error (test_target, test_prediction)   #오차 계산
# 여기서 mae=19, 예측이 평균적으로 19g 벗어났다.   
```  
---
#### 3. 문제 발생   
``` python
print(knr.score(train_input, train_input))  #출력값: 0.96
```   
#### 훈련세트로 테스트한다면 당연히 점수가 더 높아야 하는데 더 낮다.        
* 과대적합 : 훈련세트 점수 높지만 테스트 점수 굉장히 낮음       
* 과소적합 : 훈련세트보다 테스트 점수가 높음 or 두 점수 모두 낮음      

#### 4. 문제 해결   
#### 훈련세트에 더 적합하게 만든다. → 이웃의 개수를 줄인다.   
∵ 이웃의 개수가 많으면 데이터의 전반적인 패턴을 따르고 개수가 낮으면 국지적인 패턴에 민감해진다.     
``` python   
knr.n_neighbors = 3
knr.fit(train_input, train_target)   
print(knr.score(train_input, train_target))   # 0.98  
print(knr.score(test_input, test_target))    # 0.97
```
#### 훈련세트와 테스트세트 점수가 모두 높아야 하고 훈련세트 점수가 더 높아야 한다.    
## 02. 선형회귀   
### 문제: k 최근접의 한계    
``` python
print(knr.predict([[50]]))   #출력값 1033, 실제값보다 훨씬 작음   
print(knr.predict([[100]]))    #1033 으로 동일하게 예측      
```
### 원인  
![최근접이웃한계](/img/최근접이웃한계.png)         
근처이웃의 평균을 구하기 떄문에 실제값보다 낮게 예측된다.   
```python    
distances, indexes = knr.redict([[50]])
np.mean(train_target[indexes])  # = 1033 
```        
### 해결 
#### 1. 선형회귀   
#### 특성이 하나인 경우 직선을 학습하는 알고리즘   
``` python
from sklearn. linear_model import LinearRegression 
lr = LinearRegression()
lr.fit(train_input, train_target)
lr.predict([[50]])    # 출력값: 1241
```
![선형회귀](/img/1차직선선형회귀.png)     
```python
# 1차 직선: 농어무게 = a * 농어길이 + b
lr.coef_    # a 값
lr.intercept_   #b 값 
plt.plot ([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])   # 최적 직선 그래프    
```
* 기울기 = coefficient(계수) = weight(가중치)
* 모델 기반 학습 : 훈련 중에 알고리즘이 최적의 모델 파라미터를 찾는 것       
  모델 파라미터: 머신러닝 알고리즘이 찾은 값  &nbsp; &nbsp; &nbsp; ex) coef_, intercept_        
* 사례 기반 학습 : 훈련 세트를 저장하는 것이 훈련의 전부인 경우   &nbsp; ex) knn
#### 직선으로는 해결되지 않는 문제점   
무게가 0g 이하로 내려간다, 그 전보다 나아졌지만 정확도가 그렇게 높진 않다.      
#### 2. 다항회귀   
#### 최적의 곡선 구해보기       
![최적의곡선](/img/최적의곡선.png)    
``` python
# 길이^2 데이터 준비하기 
train_poly = np.column_stack((train-input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))  
# 2차방정식의 a,b,c 찾기(훈련시키기)
lr.fit(train_poly, train_target)    #타깃값은 무게로 항상 동일
lr.predict([[50**2,50]])  #출력값 1573
```
```python
print(lr.coef_)   # [1.014 -21.55] = [a값 b값]   
print(lr.inercept_)    # 116.05 = c값
```
#### 곡선 그래프 그려보기 
```python
point = np.arange(15,50)
plt.scatter(train_input, train_target) #산점도   
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)
```
![최적곡선](/img/최적곡선.png)       
#### 해결되지 않은 문제 
테스트 점수가 아직 더 높다.      
* 2차 방정식을 선형회귀로 부를 수 있을까?    
y = ax^2 + bx + c 에서 독립변수 x 와 종속변수 y는 비선형적이다. 하지만 a,b,c 계수들과는 선형적이다. 입력변수 x 를 다항식으로 변형한 후에 선형회귀를 수행하는 선형회귀의 확장된 형태이다.        
이 단원에서 길이^2 를 왕길이라고 치환해보면 무게 = 1.01*왕길이 -21.6 * 길이 + 116.05 이므로 무게는 왕길이와 길이의 션형관계로 표현할 수 있다.   
## 03. 특성 공학과 규제   
### 문제     
다항회귀에서 특성을 길이로만 하고 무게를 예측했을 때 테스트 점수가 훈련 점수보다 높았다.   
### 해결 : 다중회귀    
훈련세트의 정확도를 더 높이기 위해서 특성을 더 많이 사용한다. 농어의 길이. 높이, 두꼐 를 특성으로 사용하고 기존의 특성을 통해 새로운 특성을 뽑아내겠다. 이걸 특성공학이라고 하는데 예를 들면 '농어길이' x '농어높이' 라는 새로운 특성을 만들어내는 것이다.    
#### 1. 데이터 준비하기    
이전과 달리 판다스를 통해서 인터넷에서 바로 데이터를 불러온다. 그런 다음 넘파이 배열로 바꿔준다.       
``` python
import pandas as pd
df = pd.read_csv('https://bit. ly/perch_csv')  
perch_full = df.to_numpy()    #길이, 높이, 넓이  
perch_weight = np.array(농어무게 데이터)  #무게(타깃데이터)
train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)  # 훈련,테스트 세트     
```
#### 2. 새로운 특성 만들기   
#### 사이킷런 변환기 
사이킷런이 제공하는 특성을 만들거나 전처리하는 클래스, fit() 및 transform() 메서드를 제공한다.    
``` python  
poly = PolynomialFeatures (include_bias=False)  #괄호 안 없어도 됌, 특성에서 1을 삭제하는 것임    
poly.fit (train_input)  #훈련시킴
train_poly = poly. transform(train_input)    #기존 특성들로 새특성 만듦
test_poly= poly.transform(test_input)   #훈련세트와 동일한 규칙적용
```
* poly.fit(test_input) 안하는 이유?        
훈련세트와 테스트세트에 동일한 변환을 시켜주는 것이 좋다. 일관성을 유지하고 모델의 일반화 성능을 정확하게 평가하기 위함이다. 모델이 새로운 데이터인 테스트 세트에 어떻게 잘 적용되는지 평가하는 것이므로 테스트 세트도 훈련세트와 동일한 변환을 적용해야 한다.         

``` python
# 참고: 특성을 어떻게 만들었는지 확인하는 방법             
poly. get_feature_names_out() 
# 출력값
['x0','x1','x2','x0^2','x0 x1','x0 x2','x1^2','x1 x2','x2^2']  
```
#### 3. 다중 회귀 모델 훈련하기     
``` python   
lr.fit(train_poly, train_target)
#훈련세트 점수 0.99, 테스트세트 점수 0.97 문제해결
```
---
### 규제      
: 머신러닝의 과도한 학습을 막는 것         
![규제](/img/규제개념.png)         
→ 기울기를 줄여 보편적인 패턴을 학습을 하게 만듦         
```python
#과도한 학습인 경우   
poly = PolynomialFeatures (degree=5, include_bias=False)     #5제곱까지     
poly.fit(train_input)
train_poly = poly.transform(train_input)   
test_poly = poly.transform(test_input)  
print(train_poly.shape)   #특성개수 55개 
print(lr.score(test_poly, test_target))  #테스트점수 -144
```
#### 규제 전에 정규화시키기    
``` python
# 표준점수 직접 구하는 방법 말고 정규화 시켜주는 클래스 활용     
from sklearn.preprocessing import StandardScaler   
ss = StaindardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)   
```
#### 1. 릿지(ridge)       
회귀 계수 제곱합에 패널티를 부여한다. 그래서 모델이 계수를 작게 유지해 과대적합을 방지하게 만든다. alpha 값이 커지면 규제의 강도가 커져서 계수 값을 더 줄인다.     
```python
# alpha 값 변경하면서 최적 규제 정도 찾기        
import matpplotlib.plyplot as plt
train_score=[]
test_score=[]   
# alpha 값 차례대로 점수 기록하기   
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    ridge = Ridge(alpha=alpha)   #릿지 모델 만들기   
    ridge.fit(train_scaled, train_target)    
    train_score.append(ridge.score(train_scaled,train_target)) # 훈련세트 점수   
    test_score.append(ridge.score(test_scaled,test_target))  # 테스트 점수    
#그래프호 점수들 확인해보기    
plt.plot(np.log10(alpha_list), train_score)    
plt.plot(np.log10(alpha_list), test_score)
plt.show()
```
![릿지최적찾기](/img/릿지알파.png)     
최적의 알파 값은 0.1이다. 0.1 이전은 테스트 점수가 낮은 과대적합이고 0.1이후는 두 점수 모두 낮아지므로 0.1이 최적이다.       
```python
# 최적 릿지 모델로 훈련시키기     
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
```  
#### 2. 라쏘(lasso)      
회귀 계수 절댓값 합에 패널티를 부여한다. 특정 계수에 0을 부여할 수 있어 특성들을 선택적으로 사용할 수 있다.     
```python
train_score=[]
test_score=[]  
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    #라쏘모델에서는 반복횟수가 적으면 경고가 뜰 수 있어서 반복횟수를 10000으로 지정함
    lasso = Lasso(alpha=alpha, max_iter=10000)     
    lasso.fit(train_scaled, train_target)
    train_score.append(lasso.score(train_scaled, train_target))  
    test_score.append(lasso.score(test_Scaled, test_target))  
# 그패프 그리기 
plt.plot(np.log10(alpha_list), train_score)    
plt.plot(np.log10(alpha_list), test_score)
```
![라쏘최적찾기](/img/라쏘알파.png)     
최적 alpha값은 10이다.         
``` python
#최적모델로 훈련시키기   
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)   
```
```python
# 계수를 0으로 만든 개수가 몇개인지 확인하는 방법   
print(np.sum(lasso.coef == 0))    # 40
# 55개 특성 중 사용된 특성은 15개
```
#### 추가지식     
alpha 값처럼 모델이 학습하는 게 아니라 우리가 지정해줘야 하는 값을 하이퍼파라미터라고 한다. 하이퍼파라미터는 클래스와 메서드의 매개변수로 표현된다.   










 







