---
layout: post
title:  "ML 6주차: 혼공머신 6단원"
date:   2024-08-28 12:00
categories: ML_Study
---
# 06. 비지도 학습
## 01. 군집 알고리즘   
### 상황
사진을 종류별로 분류하기 → 사진의 픽셀값을 평균낼까?
### 해결  

### 이미지 살펴보기     
``` python
# 데이터 
import numpy as np
import matplotlib.pyplot as plt
fruits = np.load('fruits_300.npy')

# 데이터 배열의 크기 
print(fruits.shape)
```
![shape](/img/이미지shape.png)   

출력값이 (300,100,100)이다. (샘플의 개수, 이미지 높이, 이미지 너비) 을 나타낸다. 픽셀은 넘파이 배열 원소 하나에 대응한다.     

``` python
# 첫번째 이미지의 첫번째 행 출력
print(fruits[0,0,:])

# 첫번째 사진 확인하기 
plt.imshow(fruits[0], cmaps='gray')
plt.show

```
![pixel](/img/첫번째%20픽셀값.png)    
![pic](/img/사과반전.png)       
흑백 사진을 표현함으로 0~255까지의 정숫값을 가진다. 0에 가까울 수록 검은색에 가까워진다. 
컴퓨터는 높은 값, 흰색에 집중하기 떄문에 이 흑백 이미지는 넘파이 배열로 변환할때 반전시킨 것이다. 

```python
# 원본처럼 다시 반전 
plt.imshow(fruits[0], cmap='gray_r')
```
![[pic](/img/사진반전반전.png)]   
다시 반전시켰으므로 빍은 부분이 0에 가깝다.     

``` python
# 나머지 과일 종류 이미지 그리기 
fig, axs = plt.subplots(1,2)  #1행 2열로 서브플롯 2개(≒막대그래프 여러개)
axs[0].imshow(fruits[100], cmap='gray_r')
axs[1].imshow(fruits[200], cmap='gray_r')
plt.show()
```
### 픽셀값 분석
```python
# 100*100 이미지를 크기 10000인 1차원 배열로 변경
apple =  fruits[0:100].reshape(-1,100*100)
pineapple =  fruits[100:200].reshape(-1,100*100)
banana =  fruits[200:300].reshape(-1,100*100)

# 픽셀의 평균값 구하는 식
# axis=1 열방향으로 구해야 하나의 이미지(같은행) 픽셀값 구할 수 있음
apple.mean(axis=1)  

# 히스토그램으로 표현
plt.hist(np.mean(apple, axis=1), aplha=0.8)
plt.hist(np.mean(pineapple, axis=1), aplha=0.8)
plt.hist(np.mean(banana, axis=1), aplha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
```
![그래프](/img/과일픽셀히스토.png)      

#### 문제 : 사과와 파인애플 구분 안됌 

#### 해결 : 픽셀 별 평균값 구하기 
``` python
fig, axs = plt.subplots(1, 3, figsize=(20,5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
```
![그래프](/img/과일픽셀별평균값.png)     
사과: 아래로 갈수록 값 높음/ 파인애플: 고르고 높음/ 바나나: 중앙이 높음    

```python
# 위 그래프를 이미지로 출력하기 
apple_mean = np.mean(apple, axis=0).reshape(100,100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100,100)
banana_mean = np.mean(banana, axis=0).reshape(100,100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, camp='gray_r')
axs[1].imshow(pineapple_mean, camp='gray_r')
axs[2].imshow(banana_mean, camp='gray_r')

plt.show()
```
![이미지](/img/픽셀별이미지.png)     

### 군집   
비슷한 샘플끼리 모으는 작업, 이때 이 그룹을 클러스터라고 한다.    

```python
# 절댓값 평균 구하기 
abs_diff = np.abs(fruits - apple_mean)   # 절댓값
abs_mean = np.mean(abs_diff, axis=(1,2))   # 평균 

# 오차가 작은 샘플(가장 비슷한 사과 이미지)
apple_index = np.argsort(abs_mean)[:100]  #작은 순서대로 100개 
fig, axs = plt.subplots(10, 10, figsize=(10,10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
```
![사과](/img/사과100개.png)       
apple_mean 과 가까운 사진 100개가 모두 사과로 나왔다. 전체 이미지들에서 사과이미지 분리하기 성공

## 02. k-평균    
### 상황
사진에 어떤 과일이 있는 줄 모를때 과일을 종류별로 모으기(비지도학습)  
### 해결
### k-평균 알고리즘 

1. 무작위로 k개 클러스터 중심 정하기 
2. 각 샘플에서 가장 가까운 클러스터 중심 찾아서 그 클러스터의 샘플로 변경    
3. 클러스터 중심 변경 (클러스터에 있는 샘플의 평균값) 
4. 중심 변화 없을 때까지 2번으로 돌아가 반복    

![개념](/img/k-평균군집개념.png)  

```python
# 데이터 
import numpy as np
fruits = np.load('fruits_300.npy') 
fruits_2d = fruits.reshape(-1, 100*100)

# k평균 
from sklearn.cluster import KMeans
km = KMeans(n_cluster=3, random_state=42) #클러스트 3개 
km.fit(fruits_2d)
```
``` python
# 군집된 결과 확인  
print(km.labels_)  # 순서와 값에는 의미 없음 
print(np.unique(km.labels_, return_counts=True))
```
![위](/img/k평균라벨값.png)           
![아래](/img/k레이블개수.png)      

```python
# 클러스터에 있는 이미지 구하기 
import matplotlib.pyplot as plt 
# 이미지 구하는 함수 만들기 
def draw_fruits(arr, ratio=1):
    n = len(arr)   # 샘플 개수 
    rows = int(np.ceil(n/10))   # 한 행에 10개씩, 행개수 
    cols = n if rows <2 else 10  # 열 개수 

    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)  

    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    
    plt.show()

# 이미지 그리기
draw_fruits(fruits[km.labels_==0])
draw_fruits(fruits[km.labels_==1])
draw_fruits(fruits[km.labels_==2])
```
![레이블2](/img/레이블2.png)     
레이블이 2인 클러스터이다. 레이블 0과 1은 모두 같은 과일들로 그룹화되었다.         

```python
# 클러스터 중심을 이미지로 출력
draw_fruits(km.cluster_centers_.reshape(-1,100*100), ratio=3)

# 샘플과 클러스터 중심까지 거리 
# 2차원 (1,10000) 으로 구하기
# fruits_2d[100]은 1차원
print(km.transform(fruits_2d[100:101])) 

# 가장 가까운 클러스터 중심
print(km.predict(fruits_2d[100:101])) 

# 알고리즘 반복횟수 
print(km.n_iter_)   #출력값:3
```
![거리](/img/클러스터중심거리.png)        
샘플과 클러스터 0,1,2와의 거리다. 가장 가까운 클로스터 중심 출력하면 [2] 나옴   

### 단점
클러스터 개수를 사전에 지정해야 한다.   

### 해결 : 엘보우 
최적 클러스터 개수 찾기      

![개념](/img/최적클러스터.png)       
이니셔란 클러스터 중심과 샘플사이의 거리 제곱합이다. 클러스터에 속한 샘플이 중심과 가까울 수록 이너셔 값은 줄어든다. 클러스터 개수를 늘려도 클러스터의 밀집정도가 개선되지 않는 지점이 최적이다.     

``` python
# 최적 클러스터 값 찾기 
inertia = [] 
for k in range(2,7):
    km = KMeans(n_clusters = k, random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)

# 그래프로 그리기
plt.plot(range(2,7), inertia)
plt.show()
```
![그래프](/img/최적클러스터실제.png)     
최적 k=3

## 03. 주성분 분석    
### 상황

### 차원 축소   
데이터를 잘 나타내는 특성을 선택해 데이터 크기를 줄인다. 손실이 있지만 원본으로 복원도 가능하고 지도학습의 모델 성능도 향상시킨다.    
#### 차원
데이터가 가진 속성을 특성이라고 한다. 머신러닝에서 특성을 차원이라고도 한다. 
* 1차원 배열 : 차원 = 특성의 수 = 원소개수          
* 다차원 배열 : 축의 개수     

### 주성분 분석 PCA
#### 주성분    
데이터의 분산(퍼져있는 정도)를 잘 설명하는 방향        

<div style="overflow: hidden;">
    <img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/주성분1.png?raw=true" alt="왼쪽 이미지" style="float: left; width: 45%;">
    <img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/주성분2.png?raw=true" alt="오른쪽 이미지" style="float: right; width: 45%;">
</div>  

* 주성분(예:(2,1)) 원소의 개수 = 특성 개수 = 2
* 주성분으로 바꾼 데이터는 차원이 줄어든다. 
* 분산이 가장 큰 방향으로 주성분 그리기 → 첫번쨰 주성분에 수직이고 분산이 가장 큰 다음 방향으로 주성분 그리기      

```python 
# 데이터 
import https://bit.ly/fruits_300-0 fruits_300.npy
import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1,100*100)  

# 주성분 분석
from sklearn.decompostition import PCA
pca = PCA(n_components=50)  # 주성분 개수 지정  
pca.fit(fruits_2d)

# 주성분 저장되어 있는 곳
pca.components_.

# 주성분 배열 크기 확인
print(pca.components_.shape)  # (주성분수, 특성수) 출력

# 주성분 그림으로 확인
draw_fruits(pca.components_.reshape(-1,100*100))
```
![그림](/img/주성분그림으로표현.png)     
원본 데이터를 주성분에 투영해 특성 개수를 10000개에서 50개로 줄일 수 있다.      

``` python
# 원본데이터 차원
print(fruits_2d.shape)  # (30, 10000) 출력

# 차원 50으로 줄이기 
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)  # (300, 50) 출력
```
특성을 50개로 줄여서 손실이 발생한다. 최대한 분산이 큰 방향으로 데이터를 투영했기 때문에 원본 데이터를 상당 부분 재구성할 수 있다.    

```python
# 원본데이터 재구성   
fruits_inverse = pca.inverse_transform(fruits_pca) 

# 특성 복원됌
print(fruits_inverse.shape)  # (30, 10000)   

# 복원데이터 이미지 확인
fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)
for start in [0, 100, 200]:
    draw_fruits(fruits_reconstruct[start:start+100])
    print("\n")
```
![복원](/img/복원데이터.png)        

#### 설명된 분산  
주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값    
``` python
print(np.sum(pca.explained_variance_ratio))

# 그래프로 확인 
plt.plot(pca.explained_variance_ratio)
```
![설명된분산](/img/설명된분산.png)      
50개 주성분으로 표현하는 총 분산 비율은 약 92%다. 첫번째 주성분의 설명된 분산이 가장 크고 처음 10개의 주성분이 대부분을 표현한다. 

### 모델 훈련 비교 
``` python
# 로지스틱 회귀
from sklearn.linear_model_selection LogisticRegression
lr = LogisticRegression()

# 타깃 (사과:0 파인애플:1 바나나:2)  
target = np.array([0]*100 + [1]*100 + [2]*100) 

# 원본데이터 교차검증   
from sklearn.model_selection import cross_validate 
scores = cross_validate(lr, fruits_2d, target)  
# → 교차검증 점수 0.99, fit_time 0.94

# 축소데이터 
scores = cross_validate(lr, fruits_pca, target) 
# → 교차검증 점수 1.0, fit_time 0.032     
```
축소데이터를 사용했을때 정확도도 높고 모델 훈련 속도 또한 빠르다.   

``` python
# 설명된 분산의 비율 지정 가능 
pca = PCA(n_components=0.5)  
pca.fit(fruits_2d)

# 비율에 도달한 주성분 개수 
print(pca.n_components_)  # 출력: 2
```
2개의 특성으로 원본 데이터 분산의 50% 표현 가능하다. 
위 모델로 원본 데이터를 변환해보겠다.   

``` python
# 원본 데이터 변환   
fruits_pca = pca.transform(fruits_2d)

# 변환 데이터 교차검증 결과 
scores = cross_validate(lr, fruits_pca, target)
# → 교차검증 점수 0.99, fit_time 0.041
```
2개의 특성으로 정확도 99% 달성했다. 위 데이터로 클러스터를 찾아보겠다.   

``` python
# k-평균 알고리즘으로 클러스터 찾기  
from sklearn.cluster import KMeans
km = Kmeans(n_cluster=3, random_state=42) 
km.fit(fruits_pca)
print(np.unique(km.lables_, return_counts=True))   
```
![클러스터](/img/변환데이터클러스터.png)    
``` python 
# 과일 이미지 출력 
for label in range(0, 3):
    draw_fruits(fruits[km.labels_ == label])
    print("\n")
```
![과일](/img/변환데이터과일이미지.png)     

``` python
# 산점도로 클러스터 표현하기 
for label in range(0, 3):
    data = fruits_pca[km.labels_ == label]
    plt.scatter(data[:,0], data[:,1])

plt.legend(['apple', 'banana', 'pineapple'])
plt.show()
```
![산점도](/img/변환데이터산점도.png)   
데이터가 2개의 특성이 있기 때문에 2차원으로 표현할 수가 있다.     
사과와 파인애플 클로스터의 경계가 가깝게 붙어있는 거 빼고 잘 분류되었다.    

