---
layout: post
title:  "ML 2주차: 혼공머신 1,2 단원"
date:   2024-07-31 12:00
categories: ML_Study
---

# 01. 나의 첫 머신러닝   
## 1. 머신러닝이란?    
* 인공지능이란?   
사람처럼 작동하는 컴퓨터를 만드는 기술    
  * 강인공지능(인공일반지능)    
    사람과 구별하기 어려움 &nbsp; ex) 터미네이터      
  * 약인공지능   
    사람 도와주는 보조 역할 &nbsp; ex) 기계번역   
  * 인공지능 역사     
  ![인공지능역사](/img/인공지능역사.png)   
* 머신러닝이란?   
데이터에서 규칙을 자동으로 학습하는 알고리즘을 연구하는 기술      
   * 사이킷런: 머신러닝 라이브러리             
   이미 구현된 알고리즘을 사용할 수 있게 함     
   * 딥러닝이란?   
   인공신경망을 기본으로 한 머신러닝 알고리즘    
     * 딥러닝 라이브러리: 텐서플로, 파이토치   
## 2. 2개 종류 분류하는 머신러닝    
* 분류: 머신러닝에서 여러 개의 종류(클래스) 중 하나를 구별해내는 것    
* 목적 : 생선의 길이,무게를 보고 도미와 빙어를 구분하는 것(이진분류)     
   * 머신러닝은 분류의 기준을 스스로 찾아서 분류함   
###  과정   
#### 1. 도미와 빙어의 특성 산점도로 확인해보기 
   ``` python    
   plt.scatter(bream_length,bream_weight) # 토미의 특성
   plt.scatter(smelt_length,smelt_Weight) # 빙어의 특성  
   plt.xlabel('length')
   plt.ylabel('weight')
   plt.show()
   ```
   ![산점도 그래프](/img/산점도%20생선.png)
#### 2. 데이터 합치기, 이차원리스트 만들기(∵ 사이킷런 사용)  
   ``` python 
   length= bream_length + smelt_length  
   weight= bream_weight + smelt_length  
   # 이차원 리스트 만들기   
   fish_data=[[l,w] for l,w in zip(length,weight)] #zip: 리스트에서 원소를 하나씩 꺼내줌
   ```
#### 3. 정답데이터 알려주기
* 기준 만들기 위해선 정답 알려줘야 함   
* 생선을 숫자로 표현: 1=도미, 0=빙어   
``` python 
#fish_data 와 하나씩 대응됌(앞35개가 도미)
fish_target=[1]*35+[0]*14 
```
#### 4. k-최근접 이웃 알고리즘 사용하기    
``` python 
from sklearn.neighbors import KNeighborsClassifier  
kn=KNeighborsClassifier()
#훈련시키기=도미를 찾기 위한 기준을 학습시키기 
kn.fit(fish_data,fish_target) 
kn.score(fish_data,fish_target) #모델평가, 정확도
```
#### k-최근접 이웃 알고리즘이란?   
새로운 데이터에 대해 예측할때 근처 데이터 정보를 살피는 것   
![최근점](/img/최근접알고리즘.png)  
``` python  
kn.predict([[30,600]]) #=1, 디폴트로 근처 5개 데이터가 무엇인지 확인   
```
#### 참고 데이터 개수 변경하면?  
``` python
kn49=KNeighborsClassifier(n_neighbors=49) #49개로 변경   
kn49.fit(fish_data,fish_target) #다시 훈련
kn49.score(fish_data,fish_target)  # 정확도 0.7 
```
&nbsp; ∵ 근처 49개 데이터를 확인하면 도미가 대다수이기 때문에 어떤 데이터를 넣더라도 도미라고 판정함
#### k-최근접 이웃 알고리즘 속성    
_fit_X: fish_data, _y: fish_target  
# 02. 데이터 다루기   
## 1. 훈련세트와 테스트세트    
* 
    ||지도학습|비지도학습|
    |------|---|---|
    |입력(데이터)|o|o|
    |타킷(정답)|o|x|
    |사용|데이터분류,정답 맞추기|데이터 파악과 변형|
* 이전 알고리즘의 문제점      
훈련한 데이터와 같은 데이터를 사용해 테스트 함 → 정확도가 항상 1임    
* 해결방안    
훈련데이터와 테스트에 쓸 데이터를 나누기   
### 과정   
* 주의: 샘플링 편향   
훈련세트에도 도미와 빙어, 테스트 세트에도 도미와 빙어가 들어가야 함    
→ 전체 데이터를 섞은 후 훈련세트와 테스트 세트로 나눠야 함    
``` python
# 샘플링 편향이 생긴 코드   
train_input=fish_data[:35]   #훈련세트 데이터
train_target=fish_target[:35]    #훈련세트 정답 
test_input=fish_data=[35:]    #테스트세트 
test_target=fish_target[35:]
```
#### 1. 넘파이 배열 만들기   
#### 넘파이 
고차원 배열로 쉽게 만들어 주는 도구 제공   
* 배열 인덱싱 : 한 번에 여러 개 원소 선택  
input_arr[[1,3]]: 2번째랑 4번째 샘플   
* 인덱싱 사용법   
input_arr[행의 인덱스, 열의 인덱스]
``` python
# 넘파이 배열로 바꾸기  
import numpy as np 
input_arr=np.array(fish_data)
target_arr=np.array(fish_target)
# 배열의 크기 확인하는 방법
print(input_arr.shape) #출력값: (샘플 수, 특성 수) 여기선 (49,2)
```
#### 2. 인덱스 만든 후 섞어주기   
#### 인덱스가 필요한 이유? 
![인풋과 타깃](/img/인풋과%20타킷.png)       
타깃과 샘플이 함께 이동해야 함으로 인덱스를 섞는게 편리   
``` python
np.random.seed(42)   # 실행할때 동일한 결과가 나오게 만드려고(실전에선 생략)
index=np.arange(49)   # 인덱스 생성
np.random.shuffle(index) # 무작위로 섞기 
```
#### 3. 훈련세트와 테스트세트 만들기  
```python
train_input=input_arr[index[:35]]
train_target=target_arr[index[:35]]
test_input=input_arr[index[35:]]
test_target=target_arr[index[35:]]     
```
#### 4. k-최근접 이용   
```python
kn.fit(train_input,train_target)    #훈랸은 훈련세트로  
kn.score(test_input,test_target)    #평가는 테스트세트로 
```
## 2. 데이터 전처리     
### 전 파트와 다르게 데이터 준비하는 방법   
```python
fish_data=np.column_stack((fish_length,fish_weight))  #튜플로 붙일 리스트 전달  
fish_target=np.concatenate((np.ones(35),np.zeros(14)))  #튜플로 붙일 리스트 전달
#훈련과 테스트 세트로 나눠주기(3:1비율)
from sklearn.model_selection import train_test_split 
train_input,test_input, train_traget, test_target= train_test_split(fish_data,fish_target,stratify=fish_target, random_state=42)  #순서확인, stratify: 타깃에 있는 도미와 빙어의 비율과 동일하게 샘플의 클래스 비율을 만듦
```
### 문제    
kn.predict([[25,150]]) 하면 빙어로 예측한다.   
```python
plt.scatter(25,150,marker='^')      
distances, indexes = kn. kneighbors ([[25, 150]]) #가까운 이웃 찾기   
plt.scatter(train_input[indexes, 0],train_input[indexes, 1],marker='D') #그래프에 이웃 표시 
```
가까운 이웃 5개 중 4개가 빙어이다.  
![문제](/img/전처리문제.png)
### 원인   
두 특성의 스케일이 다르다. 즉 x축과 y축 범위가 다르다.   
### 해결방안   
데이터 전처리를 한다. 특성값을 일정한 기준으로 맞춰 준다. 
#### 축 범위 변경   
```python
plt.xlim((0,1000))
```
#### 표준점수 활용  
##### 표준점수: 데이터가 원점에서 표준편차의 몇배만큼 떨어져있는지 나타내는 값  
#### 1. 표준점수 구하기  
##### 평균을 빼고 표준편차로 나눠주기   
``` python
mean = np.mean (train_input, axis=0) # axis=0: 행을 따라 각 열 계산   
std = np.std(train_input, axis=0)
train_scaled = (train_input - mean)/ std 
```
##### 브로드캐스팅  
모든 행에 적용하는 것  
![브로드캐스팅](/img/브로드캐스팅.png)   
#### 2. 샘플 [20,150] 도 동일한 기준으로 변환시키기  
``` python
new = ([25, 150] - mean) / std 
pit.scatter (new[0], new[1], marker='^')  #그래프에 표시할거라면  
```
#### 3. 다시 k-최근접 이웃 알고리즘에 적용  
```python
kn. fit(train_scaled, train_target)
test_scaled= (test_input - mean) / std   #테스트 세트도 전처리  
kn.score(test _scaled, test_target)  # 출력은 1
```
#### 4. 최종결과 확인  
``` python
print(kn.predict([new]))  #도미(1)로 나옴   
#그래프 확인  
distances, indexes = kn.kneighbors([new]) 
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter (new[0], new[1], marker='^') 
plt.scatter(train_scaled[indexes,0], train_scaled[indexes, 1], marker='D')
```
![스케일해결](/img/스케일해결.png)     
