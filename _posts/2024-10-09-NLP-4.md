---
layout: post
title: "NLP 4주차: transformer(1)"
date: 2024-10-09
categories: NLP
---
## GPT
generative pre-trained transformer, 텍스트를 생성하는 사전학습된 트랜스포머(머신러닝 신경망의 일종)        
text-to-voice, voice-to-text, text-to-image 등을 할 수 있음   
텍스트 조각이 주어짐 → 그 다음 단어 예측 → 포함해서 텍스트 조각으로 주어짐 → 그 다음 단어 예측      

![과정](/img/transformer귀여운개념.png)

인풋값에 대해 매개변수를 조절해가면서 출력값을 만든다. 
![과정](/img/transformer간략귀욤.png)

Tunable parameters = weights 을 여러번 반복하면서 최종단어 예측을 한다. 
![개념](/img/transformer전체반복과정.png)  
 
전체 과정을 크게 4가지로 나타낼 수 있다. 
![사진](/img/transformer4과정.png)    

 
## Transformer 내부 
### Embedding
텍스트 전체를 tokens 로 나눔 → 인베딩 → 의미가 비슷하면 벡터 비슷한 곳에 위치 
#### token & 인베딩 
토큰으로 나눈다 != 단어로 나눈다
더 작게 나뉘는데 예를 들어 ",", "ver" 이런식으로 나뉜다.    
임베딩할떄 12288 차원으로 상당히 고차원이다.     
![벡터](/img/transformer임베딩메트릭스.png)

#### 벡터
* 의미가 비슷하면 벡터끼리 가깝다
* 방향이 뜻을 가진다. 
* 두 벡터 도트 곱은 벡터들이 얼마나 잘 정렬되는지를 측정한다. 양수면 같은방향,0이면 수직, 음수면 반대방향이다. 

아래 방향은 성별의 차이를 나타낸다. 
![벡터](/img/transformer임베딩방향.png)     

plur 가 cats 와 cat 차이를 나타낸다고 하자.
![벡터](/img/transformer임베딩방향2.png)      
plur 와 다수, students 가 유사하므로 도트곱이 양수로 표현된다.      
![벡터](/img/transformer임베딩방향3.png)        


### Attention   
이 과정을 여러번 반복하면서 문맥파악이 이뤄진다. 
처음에는 각 벡터가 인베딩 메트릭스에서 뽑기 때문에 단어 의미만을 학습하지만 
학습을 반복하면서 문맥을 학습한다.   
![토큰](/img/transformer임베딩.png)  %
임베딩행렬에서 각 토큰에 해당하는 벡터를 가져온다.     
![문맥](/img/transformer임베딩문맥학습.png)   
벡터들이 문맥에 따라 변환한다.       

#### 예시 
형용사-명사간 관계 single-head attention를 예시로 들어보겠다.       multi-head attention 은 single-head 여러개가 모인건데(96개 모임) 그럼 다양한 단어간 관계를 학습할 수 있다.      
처음 :  단어의 의미 +  단어의 위치
![처음](/img/transformerattentionsingle1.png)       

매개변수 조정(MLP단게에서) 을 통해 벡터값(의미간 관계를 나타내도록)을 조정함
![가중치](/img/transformerattention2.png)

#### 매개변수 Q,K
여기서 Q는 질문, K는 질문에 답변이다.    
![Q](/img/transformerQ1.png)      
![Q](/img/transformerQ2.png)     
위와 똑같이 벡터K를 만들었을때 creature에 Q가 곱해 진 벡터와 fluffy,blue에 K가 곱해진 벡터는 서로 비슷한 위치에 있다.    
![Q,K](/img/transformerQK1.png)         
위 사진을 embeddings of fluffy,blue attend to embeddings of creature 라고 한다.            

Q,K 벡터가 서로 유사한 정도를 판단하기 위해 도트곱을 구한다.    
![도트곱](/img/transformerQK2.png)      
![도트곱](/img/transformerQK3.png)           

누가 creature 의 형용사인가? 접니다 fluffy blue              
위 사진을 **attention pattern**라고 하는데 fluffy,blue 와 creature 의 도트곱이 상당히 높다.            
이제 도트곱을 소프트 맥스를 통해 확률값을 구할것이다. 


![소프트](/img/transformerQKs.png)   
![소프트](/img/transformerQKs1.png)

여기서 뒤에 나오는 단어가 앞의 단어에 영향을 끼치면 안된다. 뒤 단어를 미리 아는 것은 단어예측을 하는데 이미 답을 갖고 있는 것과 같다. 대각선 아래가 0이 되길 원하지만 그러면 각 열의 합이 1이 될 수 없으므로 아래 값을 -무한대로 보내버린다. 이를 **masking** 이라고 한다. 

#### 매개변수 V   
단어 간 관계를 더 복잡하게 학습하는 과정이다.     
![V](/img/transformerVa.png)       
위에서 fluffy에 value metrix를 곱한 것은 creature 과 fluffy creature 벡터 차이를 설명해준다.        
![V](/img/transformerV1.png)        
소프트맥스가 적용된 값에 v벡터를 곱해준다. fluffy,blue는 값이 있지만 나머지는 없다.       
각 열마다 차이를 구해서 전체를 구한다.  
![v](/img/transformerVL.png)       


#### 매개변수 개수 
![qkv](/img/transformerQKV.png)     
Q,K는 12,288 * 128 크고 V는 12,288*12,288크기이다.     
multi-headed attention 을 병렬적으로 시행하기 떄문에 Vparams = Qparams + Kparams 인게 더 효율적이다.     

![v](/img/transformerVlower.png)  
V메트릭스를 두개로 나눌 수 있다. 앞은 value up, 뒤는 value up이고 각각 차원을 다시 높이고 낮추는 역할을 한다.        

96개의 mutli head에 대해 전부 적용된다.     
![multi](/img/transformermulti1.png)
![m](/img/transformermulti3.png)  

매개변수의 개수를 계산해보면 아래와 같다.        
![mutli](/img/transformermult4i.png)        
위 사진은 전체 가중치의 1/3정도를 설명한다.        
Attention is about 1/3 of what you need
### MLPs
매개변수의 조정이 일어난다.    
gpt3에는 1750억개의 매개변수가 있는데 아래와 같이 나뉜다. 
![매개변수](/img/transformer뭐로나뉘는지2.png)

### 촤종단어 예측
컨텍츠 사이즈 때문에 한번에 2048개만 학습가능하므로 한 번에 예측할 수 있는 단어의 수에 제한이 있다.     

최종 출력이 구해졌다면 임베딩 행렬과 최종 출력 벡터를 곱한다.   
곱한 값이 높을 수록 유사하다는 것을 의미한다. 

#### 왜? 
"The cat sat on the _(mat)" 에서 
임베딩행렬은 5만 개의 단어(이미지에서 약 50k values)와 그에 대한 벡터로 구성
출력 벡터가 생성된다. 
출력 벡터를 임베딩 행렬과 곱한다.임베딩 행렬은 각 단어의 벡터를 가지고 있고, 그 벡터와 출력 벡터의 도트곱을 시행한다. 그럼 유사도가 구해진다. "mat"에 해당하는 벡터와 출력 벡터가 매우 유사하다면, 두 벡터의 도트 곱 결과가 크게 나혼다. 

출력벡터와 임베딩 행렬을 곱한 값을 로짓이라 한다. 이 값을 소프트맥스를 통해 확률값을 0~1사이 값으로 만든다.       
![개념](/img/transformer소프트맥스2.png)  

소프트 맥스에 T(온도)를 추가하면 아래와 같다.   
![개념](/img/transformer소프트맥스T.png) 
![개념](/img/transformer소프트맥스T2.png)        
T값이 크면 lower value 에 큰값을 준다.     
0이면 most predictable 를 고르고 T가 크면 낮은 단어가 선택될 가능성을 높인다.      
보통 2보다 큰 온도를 택하진 않는다.       
