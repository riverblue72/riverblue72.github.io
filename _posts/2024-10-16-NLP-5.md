---
layout: post
title: "NLP 5주차: transformer(2)"
date: 2024-10-16
categories: NLP
---
# nanochatgpt
## 데이터 준비 
#### 파일 읽기 → 단어책만들기 → 인코딩하기 → 테스트,검증세트 나누기 → 미니배치 학습법
```python
# 파일 읽기 
with open('input.txt','r',encoding='utf-8') as f:
    text = f.read()   
print("length of dataset in characters:", len(text))

# all character in this text(여기 파일에서 사용된 단어)
chars = sorted(list(set(text)))
vocab_size = len(chars)

# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: string → integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: integers → string
# 인코딩 디코딩 예시 
print(encode("hi there"))  # 출력 : [46, 47, 1, 58, 46, 43, 56, 43]
print(decode(encode("hi there"))) # 출력 : hi there  
```
#### 단어책을 다르게 만드는 경우 
![단어책 다른 경우](/img/nanogpt단어책다른경우.png)        
여기선 단어책을 파일에 있는 글에만 쓰인 문자들을 가지고만 만들었다. 사이즈가 65로 모든 단어가 0~64의 숫자를 갖는다. 이와 달리 위 사진에선 단어책의 크기가 50257이고 인코딩의 예시를 보면 71, 4178로 둘 간격이 크다. 

```python
# encoding entire text
import torch
data = torch.tensor(encode(text), dtype =torch.long)

# data into train and val set
n = int(0.9*len(data))
train_data = data[:n]
val_data = data[n:]  
```
```python
# chunk data
block_size = 8   # 한번에 예측시킬 개수 
x = train_data[:block_size]
y = train_data[1:block_size+1]  # 단어별로 한칸씩 뒤가 정답이니까

for t in range(block_size):
    context = x[:t+1]
    target = y[t]
    print(f"when input is {context} the target: {target}")
```
위 결과는 아래와 같다.   
![결과](/img/nanogptdata.png)      
```python
# entire data를 minibatch 로 
torch.manual_seed(1337)
batch_size = 4 # how many independent sequences will we process in parallel?
block_size = 8 # what is the maximum context length for predictions?

def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data  # 테스트, 검증세트에서 무작위로 배치데이터생성
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

xb, yb = get_batch('train')
print('inputs:')
print(xb.shape)
print(xb)
print('targets:')
print(yb.shape)
print(yb)

print('----')
# 위와 동일하지만 전체데이터에 대해서 
for b in range(batch_size): # batch dimension
    for t in range(block_size): # time dimension
        context = xb[b, :t+1]
        target = yb[b,t]
        print(f"when input is {context.tolist()} the target: {target}")
```
사진은 전체의 일부분만 가져왔다. 
![결과](/img/nanogptentire.png)            

## bi-grams로 셰익스피어 글 만들어보기 

```python
# bi-gram
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        # = 각 토큰이 다음 토큰의 logits을 임베딩 테이블에서 바로 읽어옴
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):
        
        # idx and targets are both (B,T) tensor of integers
        # B: batch, T:timesteps(blocksize), C(channels:임베딩벡터의 차원)
        logits = self.token_embedding_table(idx) # (B,T,C)

        if targets is None:
            loss = None
        else:
            # 타깃이 주어지면, 예측값과 타깃값 비교
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # predictions
            logits, loss = self(idx)
            # focus only on the last time step(마지막 단어에 대한 예측값만 가져옴)
            logits = logits[:, -1, :] # becomes (B, C)
            # softmax(probabilities)
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

m = BigramLanguageModel(vocab_size)
logits, loss = m(xb, yb)
print(logits.shape)   # torch.Size([32, 65])
```
#### generate 함수 
주어진 입력(idx)을 바탕으로 max_new_tokens 개수만큼 새로운 단어를 생성       
max_new_tokens만큼 반복 → 시퀀스(idx)에 대해 다음 단어의 logits(예측값)과 손실(loss)을 계산 → logits 텐서에서 마지막 시간 단계(시퀀스의 마지막 단어)에 대한 예측값만 → logits에 대해 softmax 함수를 적용하여 확률 분포로 만듦 → 다음에 나올 단어(토큰)를 샘플링 → torch.cat 함수는 주어진 텐서(idx)와 새로 생성된 단어(idx_next)를 시간 차원(dim=1)을 기준으로 이어붙임        

```python
# 최적화 : 모델의 파라미터를 점진적으로 업그레이드 
# AdamW Optimizer : 가중치 감쇠까지 해줌
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)
batch_size = 32
for steps in range(100): 

    # 데이터배치 생성
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()  # 손실에 대한 그레디언트 생성
    optimizer.step()  # 모델파라미터 업데이트 

print(loss.item())  # 이 출력값을 계속 줄여나간다. 

# bi-gram으로 shakespeare 글 만들기 

print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))
```
![결과](/img/nanogptbigramresult.png)            

## mathmetical trick in self-attention
self-attention 설명도와주기 위해서 아래를 먼저 설명한다.  
앞의 단어를 보고 뒤의 단어를 예측하는 가장 쉬운 방법은 앞의 단어들의 평균을 구하는 것이다. (단어(토큰)의 벡터 표현의 평균)
### version 1: 시간 단계별 단순 평균  
```python 
torch.manual_seed(1337)
B,T,C = 4,8,2 # batch, time, channels
x = torch.randn(B,T,C)
x.shape

# 이전 시점들(i <= t)의 값을 평균내기 
xbow = torch.zeros((B,T,C))
for b in range(B):
    for t in range(T):
        xprev = x[b,:t+1] # (t,C), 이전값
        # 평균, backofwords
        xbow[b,t] = torch.mean(xprev, 0)  
```
### version 2: 행렬 곱을 사용한 가중 평균   
```python 
# 행렬곱으로 평균구하는 예시 
torch.manual_seed(42)
a = torch.tril(torch.ones(3, 3))   
a = a / torch.sum(a, 1, keepdim=True)
b = torch.randint(0,10,(3,2)).float()
c = a @ b    ## 이러면 평균 구해짐
print('a=')
print(a)
print('--')
print('b=')
print(b)
print('--')
print('c=')
print(c)
```
![결과](/img/nanogptmeanversion2.png)     
c결과 1행은 2, 2행은 2+6평균, 3행은 2+6+6의 평균이다.             

```python 
wei = torch.tril(torch.ones(T, T))
wei = wei / wei.sum(1, keepdim=True)
xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)
torch.allclose(xbow, xbow2)
```

### version 3: Softmax를 사용한 가중 평균
각 단어에 다른 가중치를 부여한다.   
```python 
# 크기(T,T)인 하삼각행렬
tril = torch.tril(torch.ones(T, T))
wei = torch.zeros((T,T))
# 상삼각행렬 무시 
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)
# 가중치를 적용한 평균
xbow3 = wei @ x 
```