---
layout: post
title: "NLP 5/6주차: transformer(2)"
date: 2024-10-26
categories: NLP
---
# nanochatgpt
## 데이터 준비 
#### 파일 읽기 → 단어책만들기 → 인코딩하기 → 테스트,검증세트 나누기 → 미니배치 학습법
```python
# 파일 읽기 
with open('input.txt','r',encoding='utf-8') as f:
    text = f.read()   
print("length of dataset in characters:", len(text))

# all character in this text(여기 파일에서 사용된 단어)
chars = sorted(list(set(text)))
vocab_size = len(chars)

# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: string → integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: integers → string
# 인코딩 디코딩 예시 
print(encode("hi there"))  # 출력 : [46, 47, 1, 58, 46, 43, 56, 43]
print(decode(encode("hi there"))) # 출력 : hi there  
```
#### 단어책을 다르게 만드는 경우 
![단어책 다른 경우](/img/nanogpt단어책다른경우.png)        
여기선 단어책을 파일에 있는 글에만 쓰인 문자들을 가지고만 만들었다. 사이즈가 65로 모든 단어가 0~64의 숫자를 갖는다. 이와 달리 위 사진에선 단어책의 크기가 50257이고 인코딩의 예시를 보면 71, 4178로 둘 간격이 크다. 

```python
# encoding entire text
import torch
data = torch.tensor(encode(text), dtype =torch.long)

# data into train and val set
n = int(0.9*len(data))
train_data = data[:n]
val_data = data[n:]  
```
```python
# chunk data
block_size = 8   # 한번에 예측시킬 개수 
x = train_data[:block_size]
y = train_data[1:block_size+1]  # 단어별로 한칸씩 뒤가 정답이니까

for t in range(block_size):
    context = x[:t+1]
    target = y[t]
    print(f"when input is {context} the target: {target}")
```
위 결과는 아래와 같다.   
![결과](/img/nanogptdata.png)      
```python
# entire data를 minibatch 로 
torch.manual_seed(1337)
batch_size = 4 # how many independent sequences will we process in parallel?
block_size = 8 # what is the maximum context length for predictions?

def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data  # 테스트, 검증세트에서 무작위로 배치데이터생성
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

xb, yb = get_batch('train')
print('inputs:')
print(xb.shape)
print(xb)
print('targets:')
print(yb.shape)
print(yb)

print('----')
# 위와 동일하지만 전체데이터에 대해서 
for b in range(batch_size): # batch dimension
    for t in range(block_size): # time dimension
        context = xb[b, :t+1]
        target = yb[b,t]
        print(f"when input is {context.tolist()} the target: {target}")
```
사진은 전체의 일부분만 가져왔다. 
![결과](/img/nanogptentire.png)            

## bi-grams로 셰익스피어 글 만들어보기 

```python
# bi-gram
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        # = 각 토큰이 다음 토큰의 logits을 임베딩 테이블에서 바로 읽어옴
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):
        
        # idx and targets are both (B,T) tensor of integers
        # B: batch, T:timesteps(blocksize), C(channels:임베딩벡터의 차원)
        logits = self.token_embedding_table(idx) # (B,T,C)

        if targets is None:
            loss = None
        else:
            # 타깃이 주어지면, 예측값과 타깃값 비교
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # predictions
            logits, loss = self(idx)
            # focus only on the last time step(마지막 단어에 대한 예측값만 가져옴)
            logits = logits[:, -1, :] # becomes (B, C)
            # softmax(probabilities)
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

m = BigramLanguageModel(vocab_size)
logits, loss = m(xb, yb)
print(logits.shape)   # torch.Size([32, 65])
```
#### generate 함수 
주어진 입력(idx)을 바탕으로 max_new_tokens 개수만큼 새로운 단어를 생성       
max_new_tokens만큼 반복 → 시퀀스(idx)에 대해 다음 단어의 logits(예측값)과 손실(loss)을 계산 → logits 텐서에서 마지막 시간 단계(시퀀스의 마지막 단어)에 대한 예측값만 → logits에 대해 softmax 함수를 적용하여 확률 분포로 만듦 → 다음에 나올 단어(토큰)를 샘플링 → torch.cat 함수는 주어진 텐서(idx)와 새로 생성된 단어(idx_next)를 시간 차원(dim=1)을 기준으로 이어붙임        

```python
# 최적화 : 모델의 파라미터를 점진적으로 업그레이드 
# AdamW Optimizer : 가중치 감쇠까지 해줌
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)
batch_size = 32
for steps in range(100): 

    # 데이터배치 생성
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()  # 손실에 대한 그레디언트 생성
    optimizer.step()  # 모델파라미터 업데이트 

print(loss.item())  # 이 출력값을 계속 줄여나간다. 

# bi-gram으로 shakespeare 글 만들기 

print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))
```
![결과](/img/nanogptbigramresult.png)            

## mathmetical trick in self-attention
self-attention 설명도와주기 위해서 아래를 먼저 설명한다.  
앞의 단어를 보고 뒤의 단어를 예측하는 가장 쉬운 방법은 앞의 단어들의 평균을 구하는 것이다. (단어(토큰)의 벡터 표현의 평균)
### version 1: 시간 단계별 단순 평균  
```python 
torch.manual_seed(1337)
B,T,C = 4,8,2 # batch, time, channels
x = torch.randn(B,T,C)
x.shape

# 이전 시점들(i <= t)의 값을 평균내기 
xbow = torch.zeros((B,T,C))
for b in range(B):
    for t in range(T):
        xprev = x[b,:t+1] # (t,C), 이전값
        # 평균, backofwords
        xbow[b,t] = torch.mean(xprev, 0)  
```
### version 2: 행렬 곱을 사용한 가중 평균   
```python 
# 행렬곱으로 평균구하는 예시 
torch.manual_seed(42)
a = torch.tril(torch.ones(3, 3))   
a = a / torch.sum(a, 1, keepdim=True)
b = torch.randint(0,10,(3,2)).float()
c = a @ b    ## 이러면 평균 구해짐
print('a=')
print(a)
print('--')
print('b=')
print(b)
print('--')
print('c=')
print(c)
```
![결과](/img/nanogptmeanversion2.png)     
c결과 1행은 2, 2행은 2+6평균, 3행은 2+6+6의 평균이다.             

```python 
wei = torch.tril(torch.ones(T, T))
wei = wei / wei.sum(1, keepdim=True)
xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)
torch.allclose(xbow, xbow2)
```

### version 3: Softmax를 사용한 가중 평균
각 단어에 다른 가중치를 부여한다.   
```python 
# 크기(T,T)인 하삼각행렬
tril = torch.tril(torch.ones(T, T))
wei = torch.zeros((T,T))
# 상삼각행렬 무시 
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)
# 가중치를 적용한 평균
xbow3 = wei @ x 
```

### version 4: self-attention  
```python
torch.manual_seed(1337)
B,T,C = 4,8,32 # batch, time, channels
x = torch.randn(B,T,C)

#single head
head_size = 16
key = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
k = key(x)   # (B, T, 16)  # what do i contain
q = query(x) # (B, T, 16) # what am i looking for
wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)  # Q와 K의 내적

tril = torch.tril(torch.ones(T, T))
wei = wei.masked_fill(tril == 0, float('-inf'))  # 마스킹해줌, 하삼각행렬
wei = F.softmax(wei, dim=-1)
```
![masking](/img/transformermasking.png)     
위 사진은 wei[0]으로 마스킹된 것을 확인할 수 있다.  

#### Attention
* 통신 메커니즘으로 각 노드(토큰)들이 서로 정보를 주고 받는다.      
* 공간 개념이 없고 벡터로만 작동한다(단어 순서와 위치정보 모름). 그래서 positional 인코딩 해줘야 한다.     
* 다른 배치에 있는 데이터들은 소통하지 않는다. 같은 배치내에서만 서로 정보를 교환한다. 병렬 처리와 속도 효율성을 높이기 위해서이다.    
*  마스킹을 사용해 현재 정보가 미래정보를 참고하지 못하도록 한다. 디코더 attention 블럭이라고도 불린다. 디코더는 과거 정보만을 기반으로 새로운 토큰을 생성하지만 인코더는 모든 토큰이 정보를 주고 받는다. 
* self 는 q,k,v가 모두 같은 소스 x에서 만들어졌다는 걸 의미한다. "cross attention"은 q가 x에서 오고 k,v가 y(외부소스)에서 온다. 
* Scaled attention 이다. Q(쿼리)와 K(키)가 단위 분산을 가질 때, 어텐션 가중치인 wei도 단위 분산을 가지도록 한다. 

### Transformer Architecture
![아키텍처](/img/transformerach.png)            
#### 인코더(왼쪽)
* Input Embedding: 입력 텍스트가 벡터로 변환
* Positional Encoding: 순서 정보를 더함
* Multi-Head Self-Attention: 입력의 각 토큰이 다른 모든 토큰과 상호작용
* Feed Forward: 각 토큰에 독립적으로 비선형 변환을 적용
* Add & Norm: 각각의 결과에 대한 잔차 연결과 정규화를 수행
위가 하나의 블록이고 이 블록이 여러층 쌓인다.(여러번 과정을 반복한다.)      

#### 디코더(오른쪽)
* Output Embedding: 출력 토큰(shifted right)으로 시작
* Masked Multi-Head Attention: 자기회귀적 구조이고, 미래의 토큰을 보지 못하게 마스킹
* Multi-Head Cross Attention: 인코더에서 나온 출력과 상호작용하여 정보 결합
* Feed Forward: 출력 토큰에 비선형 변환을 적용
디코더는 인코더에서 얻은 정보를 활용해 최종 출력을 예측한다.           
GPT는 자가회귀적(autoregressive) 방식으로 동작하는 디코더 기반 구조이다. 

#### Residual block  
![개념](/img/transformerresidual.png)      
잔차 블록(Residual Block)의 다양한 변형이다. Residual이란 딥러닝에서 잔차, 예측값과 실제값의 차이를 의미한다. 차이를 네트워크가 학습할 수 있도록 입력을 그대로 전달하여, 복잡한 변환 대신 입력과 출력의 차이를 모델이 학습하게 한다. 이를 통해 깊은 네트워크에서도 기울기 소실 문제를 줄인다.   

#### Layer Normalization 
레이어 정규화는 딥러닝에서 모델의 각 레이어에 있는 뉴런의 출력을 정규화하는 기법이다. 배치 크기에 의존하지 않고, 각 샘플의 출력을 기준으로 정규화한다.          
Add & Norm 부분에 해당하고 잔차 연결로 더해진 출력에 정규화를 적용한다. 

#### Dropout
 과적합(overfitting)을 방지하기 위해 학습 과정에서 일부 뉴런을 무작위로 비활성화하여 특정 뉴런에 의존하는 것을 줄이고 일반화된 패턴을 학습하게 한다. 


```python
class FeedFoward(nn.Module):
    """ a simple linear layer followed by a non-linearity """

    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # residual 
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),  # residual 
            nn.Dropout(dropout),  # dropout
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    """ Transformer block: communication followed by computation """

    def __init__(self, n_embd, n_head):
        # n_embd: embedding dimension, n_head: the number of heads we'd like
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)   #layernorm
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))  #residual
        x = x + self.ffwd(self.ln2(x))
        return x
```