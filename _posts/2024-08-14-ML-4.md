---
layout: post
title:  "ML 4주차: 혼공머신 4단원 "
date:   2024-08-14 12:00
categories: ML_Study
---
# 04. 다양한 분류 알고리즘    
## 01. 로지스틱 회귀   
### 상황   
럭키백에 들어간 생선의 크기, 무게가 주어졌을때 럭키백에 있는 7개 생선에 대한 확롤 구하기       
### knn 활용하는 과정   
![최근접으로확률구하기](/img/최근접확률.png)   
knn 사용해서 어떤 종일 확률이 어떻게 되는지 확인가능할 것이다.       
#### 1.데이터 준비하기    
``` python  
import pandas as pd  
fish = pd.read_csv('https://bit.ly/fish_csv')   
```
데이터 일부분       
![데이터](/img/로지스틱회귀데이터준비.png)      
```python
# Species 열을 타깃으로 만들고 나머지 5개 열을 입력데이터로 만들기    
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()    
fish_target = fish['Species'].to_numpy()        
``` 
fish_input 데이터의 일부분     
![넘파이배열](/img/데이터에서넘파이로.png)
``` python
# 훈련세트,테스트세트 나누기    
from sklearn.model_selection import train_test_split  
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)
# 표준화 전처리   
from sklearn.preprocessing import StandardScaler  
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
* 참고
``` python
# 특정 열데이터 종류 확인하는 방법     
print(pd.unique(fish['Species']))    #7개 종 이름 출력됌  
```
#### 2. k 최근접으로 이웃 확률 확인하기   
```python
# knn 훈련시키기
from sklearn .neighbors import KNEighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)
# 다중분류 : 문자열로 된 타깃값 사용됌 
# knn 타깃값 확인해보기(알파벳 순으로 정렬됌)
print(kn.classes_)  # 출력: ['Bream' ' Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
# 5개 타깃값 예측값 확인해보기   
print(kn.predict(test_sclaed[:5])) # 출력: ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']   
# 5개 클래스별 확률값 확인해보기   
import numpy as np  
proba = kn.predict_proba(test_scaled[:5])  
print(np.round(proba, decimal=4)) #소수 4째자리까지 표기  
```
![확률확인](/img/최근접확률확인.png)    
1번째 열이 Bream 열, 2번째 열이 Parkki 일 확률이다. 4번째 샘플을 확인해보면 이웃 3개 중에 2개는 Perch, 1개는 Roach 인 것을 확인할 수 있다.    
이웃이 3개인 최근접 이웃을 사용하면 가능한 확률이 4개 뿐이다. 0,1/3,2/3,1 만으로는 부족하다.   
```python
# 4번째 열 이웃 3개 확인하기   
distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])    # 출력: [['Roach' 'Perch' 'Perch']]
```
---
### 로지스틱 회귀  
회귀분석과 동일한 형식의 선형방정식을 학습한다. 하지만 회귀가 아닌 분류를 시행한다. z값을 선형방정식을 통해서 구하고 시그모이드 함수를 활용해 분류를 시행한다. 이진분류일 경우 0.5보다 크면 양성클래스, 작으면 음성클래스이다.      
![z](/img/z.png) ![시그모이드](/img/시그모이드.png)          
#### 로지스틱 회귀로 이진분류    
```python
# 도미와 빙어인 행을 가져오기     
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt') # 도미와 빙어일 경우 True
train_bream_smelt = train_scaled[bream_smelt_indexes]   #불리언 인덱싱   
target_bream_smelt = train_target[bream_smelt_indexes]
#로지스틱회귀 모델 훈련시키기  
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```
* 참고      
```python
# 어떻게 훈련됐는지 확인해보기  
lr.predict(train_bream_smelt[:5])      
lr.predict_proba(train_bream_smelt[:5])
```
![이진분류예측](/img/이진분류로지스틱예측.png) ![이진분류확률](/img/로지스틱이진분류확률값.png)       
```python
# z 방정식 계수값 확인해보기     
print(lr.coef, lr.intercept_)  
# 출력: [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
```
![계수값](/img/로지스틱이진분류z.png)      
``` python
# 5개 데이터들의 z값
decisions = lr.decision_function(train_bream_smelt[:5])
# 시그모이드 함수 → 확률값 
from scipy.special import expit
print(expit(decisions))
```
![확률값](/img/시그모이드함수이진분류.png)    
predict_proba에서 구했던 양성클래스(Smelt)가 되는 확률값과 동일하다. 
#### 로지스틱회귀로 다중분류         
``` python
lr = LogisticRegression(C=20, max_iter=1000)  #로지스틱 규제는 C, 작을수록 규제 커짐
lr.fit(train_scaled, train_target) 
print(lr.score(train_scaled, train_target)) #과대적합,과소적합인지 체크해봄   
print(lr.score(test_scaled, test_target))
```
```python
#5개행 어떻게 예측했는지 확인해보기 
print (lr.predict(test_scaled[:5]))   #클래스 
proba = lr.predict_proba(test_scaled[:5])  #클래스 별 확률   
print (np.round (proba,decimals=3)) 
```
![클래스](/img/다중분류5개클래스.png) ![확률](/img/다중분류5개확률.png)     
```python
# 다중분류일떄 선형방정식 확인해보기  
print(lr.coef_.shape, lr.intercept_.shape)   
```
이때 출력값은 (7,5) (7,) 이다. 5개의 특성을 사용하므로 coef_ 열의 개수는 5개이다. coef_ 와 intercept_의 행 모두 7개인 것은 z값이 7개라는 뜻이다. 즉 다중분류는 클래스마다 z값을 계산한다. 시그모이드함수를 사용했던 이진분류와 달리 다중분류는 소프트맥스 함수를 통해 z값을 확률로 바꾼다. 
``` python
# z값 확인해보기 
decisions = lr.decision_function(test_scaled[:5])
# 소프트맥스 함수 → 확률값
from scipy.special import softmax 
proba = softmax (decision, axis=1) 
print(np.round(proba, decimals=3))
```
![소프트맥스 확률](/img/다중분류5개확률.png)    
axis=1 로 지정해 각 행별로 즉 각 샘플별로 소프트맥스를 계산하게 한다. 위에서 구했던 클래스별확률값과 소프태맥스를 통해 구한 값이 동일하다. 

#### 참고: 소프트맥스   
여러개 선형방정식의 출력값 z을 0~1 사이로 압축해 전체 합이 1이 되도록 만든다. 이때 지수함수를 활용하기 때문에 정규화된 지수함수라고도 부른다.      
z = [1.2, 0.9, −1.1, 2.3, 0.5, 1.0, −0.7] 인 경우   
1. 지수함수에 적용  
![과정1](/img/소프트맥스1.png)                   
2. 지수함수 값 합 구하기 
![과정2](/img/소프트맥스2.png)       
3. 소프트 맥스 확률 값 = 전체 합으로 각자의 값 나눈값
![과정3](/img/소프트맥스3.png)            
## 02. 확률적 경사 하강법    
### 상황    
데이터가 한 번에 주어지지 않고 나중에 데이터가 추가되는 경우, 새로운 데이터에 대해서만 학습시킬 수 있을까?     
### 해결: 점진적 학습
### 확률적 경사 하강법  
경사 하강: 산을 내려갈 때 가장 가파른 길을 찾아서 원하는 지점에 도달하는 과정      
확률적: 훈련세트에서 랜덤하게 하나의 샘플을 고르는 것    
→ 훈련세트에서 랜덤하게 하나의 샘플을 선택해 가파른 경사를 조금 내려간 후 다시 랜덤하게 샘플 고르는 과정 반복   
이때 훈련세트를 한 번 모두 사용하는 과정을 에포크라고 한다.      

샘플을 몇개 꺼내냐에 따라 미니배치 경사하강법과 배치경사하강법도 존재한다. 
![개념그림](/img/확률경사하강개념그림.png)        
### 산 = 손실함수   
손실함수는 머신러닝 알리즘의 엉터리 정도를 측정하므로 값이 작을 수록 좋다. 손실함수를 산으로 비유했을 때 손실함수의 최솟값을 향해 내려가는 것이다. 하지만 어떤 값이 최솟값인진 모르고 우리가 가능한 많이 찾아본 후 만족할 수준이면 멈추는 것이다.    

### 로지스틱 손실 함수 =  연속적인 손실 함수   
연속적인 손실함수를 위해 예측확률을 사용한다.    
![손실확률](/img/손실확률.png)       
* 타깃이 양성 클래스일 경우: 예측확률 x 1    
* 타깃이 음성 클래스일 경우: (1-예측확률) x 1 

![로지스틱손실함수](/img/로지스틱손실함수.png)      
위 그림이 로지스틱 손실함수이다. 이진 크로스엔트로피 손실 함수라고도 한다.       
``` python
# 데이터 준비 
import pandas as pd  
fish = pd.read_csv('https://bit.ly/fish_csv') 

# 훈련세트,테스트세트로 나누기 
from sklearn.model_selection import train_test_split  
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

# 전처리 
from sklearn.preprocessing import StandardScaler  
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
``` python
# 경사하강법 분류 클래스    
from sklearn.linear_model import SGDClassifier  
# 로지스틱 손실함수 지정
sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)  
```
```python
# 정확도 점수가 낮아서 점진적 학습(한번 추가학습)
sc.partial_fit(train_scaled, train_target)
```
![에포크](/img/에포크값개념.png)     
에포크 횟수가 많을 수록 훈련세트 학습의 정도가 많아진다. 하지만 어느 순간부터는 과대적합되어 테스트 점수가 떨어지기 때문에 그전에 훈련을 종료시켜야 한다. 이걸 조기종료라고 한다.       
```python
# 에포크값 찾기 
import numpy as np
sc = SGDClassifier(loss='log', random_state=42)      
train_score = []
test_score = []
classes = np.unique(train_target)

for _ in range(0,300):
    sc.partial_fit(train_scaled, train_target, classes=classes)   #모든 클래스에 대해 학습  
    train_score.append(sc.score(train_scaled, train_target))
    train_score.append(sc.score(test_scaled, test_target))

import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.show()
```
![에포크그래프](/img/에포크값찾기.png)          
적절한 반복횟수는 100번이다.       
``` python
#에포크 100으로 학습
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target) 
```
* 참고  
SGDClassifier 은 일정 에포크 동안 성능이 향상되지 않으면 자동으로 멈춘다. 이떄 tol 매개변수를 통해 최솟값 횟수를 지정한다. 여기서는 None으로 지정하므로써 100번을 무조건 반복하도록 했다.    
loss 라는 매개변수를 통해 여러종류의 손실함수를 지정할 수 있다. 기본값은 hinge 이다.        

