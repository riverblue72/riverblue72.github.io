---
layout: post
title: "NLP 2주차: bigram 모델로 이름 생성"
date: 2024-09-25
categories: NLP
---
# bigram model
각 문자가 다음 문자를 예측하는 방식으로 모델을 학습시켜 이름을 만든다. 

## bi-grams 생성 후 빈도 세기 
### bi-grams 생성
```python
# 데이터 셋
words = open('names.txt', 'r').read().splitlines()

# bi-grams 생성
b = {}
for w in words:
  chs = ['<S>'] + list(w) + ['<E>']
  for ch1, ch2 in zip(chs, chs[1:]):
    bigram = (ch1, ch2)
    b[bigram] = b.get(bigram, 0) + 1    # 횟수 카운트

# b 확인해보기
sorted(b.items(), key = lambda kv: -kv[1])
```

b.item() 결과   
![b](/img/알파벳순서쌍갯수.png)     

```python
# 위 결과를 표로 만들기 

# 1. N 을 27 x 27 인 0으로 만듦
import torch
N = torch.zeros((27, 27), dtype=torch.int32)

chars = sorted(list(set(''.join(words))))  # 알파벳 리스트 

# 알파벳:인덱스+1 딕셔너리 
stoi = {s:i+1 for i,s in enumerate(chars)}  
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}

# 2. N 에 횟수 더하기 
for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):   # bi-grams 만들기 
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    N[ix1, ix2] += 

# 3. 그림으로 만들기 
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(16,16))
plt.imshow(N, cmap='Blues')
for i in range(27):
    for j in range(27):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha="center", va="bottom", color='gray')
        plt.text(j, i, N[i, j].item(), ha="center", va="top", color='gray')
plt.axis('off');
```
![결과](/img/알파벳표.png)   

### 알파벳 뒤에 올 알파벳 확률 구하기   
위 표를 보면 a 뒤에 n 올 확률이 높다. n 뒤에는 n이 올 확률이 높다. 높은 확률로 만들어 질 수 있는 이름은 ann 이다.    

``` python 
# 높은 확률인 알파벳 구하는 법 

# 1. 확률 구하기
p = N[0].float()  # N 첫번째 행 
p = p / p.sum()

# 2. 알파벳 구하기 
g = torch.Generator().manual_seed(2147483647)
ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()   # 인덱스 구해짐
itos[ix]  # 알파벳 구해짐
```
ix 는 14, itox[ix] 의 결과는 m 이다.      

#### multinomial 작동방식

p 가 tensor([0.6064, 0.3033, 0.0903]) 인 경우 

```python
torch.multinomial(p, num_samples=100, replacement=True, generator=g)
```

총 100개가 구해지는데 0번째 인덱스 확률이 가장 높으므로 숫자 0이 많이 출력될 것이다. 

![결과](/img/multinomail결과.png)    

``` python
# 전체 N 사용해서 이름 구하기 

# 1. 확률 구하기 
P = (N+1).float()
P /= P.sum(1, keepdims=True)

# 2. 이름 만들기  
g = torch.Generator().manual_seed(2147483647)

for i in range(5):
  
  out = []
  ix = 0
  while True:
    p = P[ix]
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))

```    

### 성능 구하기 및 높이기    

예를 들어 출력한 이름이 emma 라면 ( em 일 확률 x mm 일 확률 x ma 일 확률) 구해야 한다. log로 구한다면 log(a*b*c) = log(a) + log(b) + log(c) 이므로 더 쉽다.       

``` python

log_likelihood = 0.0
n = 0

# 이름 리스트 bi-grams로 만들어 성능평가 
for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    prob = P[ix1, ix2]
    logprob = torch.log(prob)
    log_likelihood += logprob
    n += 1

print(f'{log_likelihood=}')
nll = -log_likelihood
print(f'{nll=}')
print(f'{nll/n}')  # nlㅣ 을 이름 길이로 나눠서 정규화 
```

#### 성능 높이는 방법
W = torch.randn((27, 1)) 이런 식으로 처음 랜덤하게 가중치를 구한다. 가중치를 조정하면서 성능을 높인다.   

``` python 
# 1. bigrams 데이터셋 만들기 
xs, ys = [], []

for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    print(ch1, ch2)
    xs.append(ix1)
    ys.append(ix2)
    
xs = torch.tensor(xs)     # . e m m a
ys = torch.tensor(ys)     # e m m a .  

# 2. 신경망 초기화 
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((27, 27), generator=g, requires_grad=True)    # 무작위로 가중치 값 생성 

# 3. 성능 높이기 : 그래디언트 감소 
import torch.nn.functional as F

for k in range(1): # 그레디언트 1번만 실행 
  # foward pass
  xenc = F.one_hot(xs, num_classes=27).float() # 원핫 인코딩
  logits = xenc @ W  # 행렬곱셈으로 계산됌
  counts = logits.exp() # 로짓을 지수로 표현 
  probs = counts / counts.sum(1, keepdims=True) # 다음 올 캐릭터 확률  
  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()
  print(loss.item())
  
  # backward pass
  W.grad = None    # 그래디언트 초기화 
  loss.backward()
  
  # update 
  W.data += -50 * W.grad
```

### 최종결과 : 이름 생성
``` python
g = torch.Generator().manual_seed(2147483647)
for i in range(5):
    out = []
    ix = 0
    while True:
        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()
        logits = xenc @ W
        counts = logits.exp()
        p = counts / counts.sum(1, keepdims=True)
        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        out.append(itos[ix])
        if ix == 0:
            break
    print(''.join(out))
```



