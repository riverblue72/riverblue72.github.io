---
layout: post
title:  "ML 5주차: 혼공머신 5단원 "
date:   2024-08-21 12:00
categories: ML_Study
---
# 05. 트리 알고리즘    
## 01. 결정트리    
### 상황   
알콜, 당도, ph가 있을 때 화이트 와인(1)과 레드와인(0)으로 분류하기     
### 시도: 로지스틱 회귀모델 
```python
# 데이터에 대한 정보 알아보기   
import pandas as pd
wine =  pd.read_csv('https://bit.ly/wine-date') 
wine.info()
wine.descibe()
```
![와인인포](/img/와인인포.png)      
wine.info()의 출력값. non-null 은 누적되지 않았다는 뜻이다. Dtype은 데이터타입으로 여기선 실수형이다.       
![와인설명](/img/와인통계값.png)   
wine.descibe()의 출력값. 알코올 도수, 당도, ph의 스케일이 다름으로 표준화가 필요하다.    
``` python  
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()  
# 훈련, 테스트 세트 나누기 
from sklearn.model_selection import train_test_split  
train_input, test_input, train_target, test_tesrget = train_test_split(data, target, test_size=0.2, random_state=42)  
# 전처리 
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)  
```
``` python
# 로지스틱회귀
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
```
### 해결: 결정트리 
``` python
# 결정트리 모델 훈련
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(trandom_state=42)
dt.fit(train_scaled, train_target)
# 결정트리 그림으로 보기 
import matplotlib.pyplot as plt 
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![결정트리](/img/결정트리그림.png)      

max_depth =1 으로 설정하면 루트노드에서 자식노드가 한 층만 형성된다. filled 매개변수를 통해 노드의 색을 칠하고 feature_names 를 통해 특성의 이름을 전달한다.      

<img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/결정트리개념.png?raw=true" alt="결정 트리 개념" width="300" height="200">   
<center>
<img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/루트노드.png?raw=true" width="300" height="200"> 
</center>

<div style="overflow: hidden;">
    <img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/왼쪽노드.png?raw=true" alt="왼쪽 이미지" style="float: left; width: 45%;">
    <img src="https://github.com/riverblue72/riverblue72.github.io/blob/main/img/오른쪽노드.png?raw=true" alt="오른쪽 이미지" style="float: right; width: 45%;">
</div>           


* 특정 클래스의 비율이 높아질 수록 색깔이 점점 진해진다. 왼쪽 노드보다 오른쪽 노드에서 양성클래스의 비율이 더 높으므로 색깔이 더 진하다.           
* 리프노프(최대깊이)에서 가장 많은 클래스가 예측클래스가 된다. 이 결정트리에서는 왼쪽, 오른쪽에 도달한 샘플 모두 양성클래스로 예측된다.      
* gini는 지니 불순도이다. 노드에 하나의 클래스만 있으면 지니불순도가 0이고 가장 작아진다(순수노드). 노드에 두 클래스비율이 1:1 이면 지니불순도가 0.5로 최악이 된다.     
부모와 자식노드 사이의 불순도 차이를 정보이득이라고 하는데 정보이득이 최대가 되도록 데이터르르 나눈다.(데이터를 나누는 기준이 된다.)   
노드를 순수하게 나눌 수록 정보이득이 최대가 된다. 
![불순도](/img/지니불순도.png)    
![정보이득](/img/정보이득.png)   
```python
# 가지치기
# 노드가 많을 수록 훈련세트 점수는 높아지고 테스트 점수는 떨어짐 
# 깊이 3
dt = DecisionTreeClassifier(max_depth=3, trandom_state=42)
dt.fit(train_input, train_target)
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![결과](/img/결정트리결과.png)    

불순도를 기준으로 데이터를 나누기 때문에 표준화 전처리가 필요없다. 그림을 trian_input을 훈련시킨 것으로 그 전 그림과 특성들의 수치값이 다르다.     

```python
# 특성 중요도(어떤 특성이 유용한가)  
dt.feature_importances_
```
출력값은 [0.12345626 0.86862934 0.0079144]이다. 루트 노드와 깊이 1에서 사용한 sugar의 중요도가 가장 높다. 특성 중요도는 각 노드의 정보이득과 전체샘플에 대한 비율을 곱한 후 특성별로 더한다. 특성중요도를 모두 더한 값은 1이다.      

## 02. 교차 검증과 그리드 서치    
### 목적
테스트 세트를 사용하지 않고 모델이 과대적합인지 과소적합인지 판단하기      

### 검증세트 
훈련세트와 테스트 세트를 나누고 훈련세트의 20프로를 검증세트로 만든다.    
1. 검증세트로 모델을 평가하며 가장 좋은 모델을 고른다. 
2. 훈련세트와 검증세트를 합쳐 전체 훈련데이터로 다시 훈련한다. 
3. 테스트 세트로 최종 점수를 평가한다. 

```python
#검증세트(val_input, val_target)만들기 
#훈련세트: sub_input, sub_target 
sub_input, val_input, sub_taret, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42) 
```
### 교차 검증   
검증세트를 너무 조금 떼어놓으면 점수가 불안정할 수 있다. 이를 위해서 훈련세트에서 검증세트를 떼서 평가하는 과정을 여러번 반복하낟. 이를 교차검증이라 하고 하래 그림은 3-폴드 교차 검증이다.      
![교차검증개념](/img/교차검증그림.png)       

```python
# 교차검증 
from sklearn.model_selection import cross_validate
from sklearn.model_selection import StratifiedKFold
scores =  cross_validate(dt, train_input, train_target, cv=StratifiedKFold())  
```
cross_validate()는 훈련세트를 섞지 않기에 분할기를 따로 지정해야힌다.위에선 train_test_split()을 썼기에 분할기를 지정하지 않아도 된다.
cross_validate()는 회귀모델일땐 KFold, 분류모델일땐 StratifiedKFold 를 사용한다.     

```python
# 만약 10폴드 교차검증을 하는 경우(디폴트는 5)
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
```
StratifiedKFold는 각 폴드가 동일한 클래스 비율을 가지도록 데이터셋을 나누는 것이 목적이다. shuffle=True 라고 지정해야 데이터를 나누기 전에 섞는다.      
```python
# 검증세트 점수(테스트세트 아님 이름이랑 혼동하면 안됌)
# 교차검증에서 구하는 점수는 평균한 값이다. 
import numpy as np
print(np.mean(scores['test_score'])) 
```
### 그리드 서치   
하이퍼 파라미터는 사용자가 지정해야 하는 파라미터이다. 매개변수의 값을 변경하면서 파라미터를 바꾸는데 이때 매개변수가 여러개인 경우 매개변수를 동시에 바꿔야 한다. 그리드 서치는 이를 가능하게 해준다.      
GridSearchCV는 하이퍼파라미터 탐색과 교차검증을 한 번에 수행한다. 이때 cross_validate()를 호출할 필요가 없다. 
1. 탐색할 매개변수 지정
2. 훈련세트에서 그리드 서치를 통해 최상 매개변수 조합 찾기(최상모델 저장됌) 
3. 최상 매개변수 조합으로 전체 훈련세트 훈련

#### 매개변수 1개인 경우 
```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]} #매개변수 
gs = GridSearchCv(DecisionTreeClassifier(random_state=42,params, n_jobs=-1))  
gs.fit(train_input, train_target)

# 최적 하이퍼파라미터로 학습한 모델
dt =gs.best_estimator_
print(dt.score(train_input, train_target))
```
```python
# 최적 매개변수 저장
print(gs.best_params_)

# 교차 검증의 평균 점수
print(gs.cv_results_['mean_test_score'])

# 검증점수가 가장 높은 매개변수 
best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
```
![파라미터](/img/그리드서치최적파라미터.png)
![교차검증](/img/그리드서치교차검증점수.png) 
첫번째 파라미터에서 가장 높은 교차검증점수를 가진다. 검증점수가 가장 높은 매개변수를 출력하면 최적 매개변수를 출력한 값과 똑같이 나온다.     

#### 매개변수가 여러개인 경우 
```python
#(시작점, 끝점(포함x), 간격), range()는 정수만 가능
params = {'min_impurity_decrease' : np.arange(0.0001, 0.001, 0.0001),
'max_depth' : range(5,20,1),
'min_samples' : range(2,100,10)}

#그리드 서치 
gs = GridSearchCV(DecisionTreeClassifier(random_stae=42, params, n_jobs=-1))
gs.fit(train_input, train_target)
```
```python
# 최적 매개변수 조합
print(gs.best_params_)

#최상 교차검증점수
print(np.max(gs.cv_results_['mean_test_score']))
```
![최적파라미터](/img/그리드서치매개변수여러개최상.png)  

#### 랜덤서치 
매개변수의 값이 수치일 때 값의 범위나 간격을 정하기 어려운 경우, 너무 많은 매개변수가 있는 경우에 사용한다.    
매개변수 값 목록이 아닌, 매개변수를 샘플링할 수 있는 확률분포 객체를 전달한다.     

```python
# 균등분포에서 샘플링
from scipy.stats import uniform, randint

# 정수값 고르게 뽑음
rgen = randint(0, 10) #범위지정
np.unipue(rgen.rvs(1000), return_counts=True) #범위 안 1000개 뽑음

# 실수값 고르게 뽑음 
ugen = uniform(0,1)
ugen.rvs(10)
```
![정수](/img/랜덤정수값.png)    
0을 98개, 1을 94개 ...        

![실수](/img/랜덤실숫값.png)

```python
params = {'min_impurity_decrease' : uniform(0.0001, 0.001),
'max_depth' : randint(20,50),
'min_samples_split' : randint(2,25),
'min_samples_leaf' : randint(1,25)}

# 랜덤서치
from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisonTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)   # 100번 샘플링 
gs.fit(train_input, train_target)

# 최적 파라미터
print(fs.best_params_)
# 최상 교차검증점수
print(np.max(gs.cv_results_['mean_test_score']))
# 최종모델
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
```
## 03. 트리의 앙상블   

||정형데이터|비정형데이터|
|------|---|---|
|정렬|잘됌|잘안됌|
|예시|도미 길이무게,,|사진,노래,텍스트|
|알고리즘|앙상블 학습|신경망 알고리즘|

### 랜덤 포레스트   
앙상블 학습의 대표적 알고리즘이다. 결정트리를 랜덤하게 만들어 결정트리의 숲을 만든다.      

#### 1. 부트스트랩 샘플   
훈련 데이터에서 랜덤하게 샘플을 추출하는데 이때 뽑았던 샘플을 다시 뽑을 수 있다. 훈련세트의 크기와 같게 만듦으로 100개의 훈련세트에서 100개의 샘플을 뽑아 부트스트랩 샘플을 만든다. 

#### 2. 노드의 분할 
전체 특성 개수의 제곱근만큼 특성을 선택한다. 이 중에서 최선의 분할을 찾아 노드를 분할한다. 예를 들자면 4개의 특성이 있다면 2개의 특성을 고르고 그 중에서 최선의 분할을 시도한다.      

#### 3. 전체 
각각의 결정트리를 1,2 번 단계를 통해 구한다. 분류인 경우 각 트리의 클래스 별 확률을 평균해서 가장 높은 확률인 클래스를 예측으로 삼는다. 회구인 경우 각 트리의 예측을 평균한다.  

``` python
import pandas as pd
import numpt as np 

# 데이터 준비 
wine =  pd.read_csv('https://bit.ly/wine-date') 
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()  

# 훈련, 테스트 세트 나누기 
from sklearn.model_selection import train_test_split  
train_input, test_input, train_target, test_tesrget = train_test_split(data, target, test_size=0.2, random_state=42)  

# 교차검증, 랜덤포레스트 
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42) 
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)   #True면 검증,훈련 점수 모두 반환
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
```python
# 특성중요도
rf.fit(train_input, train_target)  
print(rf.feature_importances_)
```
![특성중요도](/img/포레스트특성중요도.png)     
![특성중요도](/img/결정트리특성중요도.png)       

위가 랜덤포레스트 특성중요도, 아래가 결정트리 특성중요도이다. 랜덤포레스트에서 샘플을 랜덤으로 가져왔기 때문에 특성중요도가 다르게 나타난다. 좀 더 다양한 특성이 훈련할 기회를 얻어 과대적합을 줄일 수 있다.      

#### OOB 점수  
부트스트랩샘플을 고를때 중복을 허용했기 때문에 선택되지 않은 데이터들이 남아있다. 이 데이터를 가지고 결정트리를 평가할 수 있다. 

``` python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

### 엑스트라 트리 
부트스트랩 샘플을 사용하지 않고 전체 훈련세트 사용한다. 노드를 분할할 때 무작위로 분할한다. 성능은 낮아지지만 무작위로 구하기 때문에 계싼 속도가 빠르다.     
``` python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_socre=True, n_jobs=-1) 
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

### 그레이디언트 부스팅   
깊이가 얕은 결정트리를 사용해 높은 일반화 성능을 가진다. 경사하강법을 통해 트리를 계속 추가하면서 가장 낮은 곳으로 이동한다.  분류에서는 로지스틱 손실함수를, 회귀에서는 평균 제곱 오차함수를 사용한다.    

```python
from sklearn.ensemble import GradientBoostingClassifier 
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1) #학습률 디폴트 0.1
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 학습률, 트리개수 증가 > 성능향상
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42) 
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
![첫트](/img/그레이디언트첫트.png)    
![이트](/img/그레이디언트이트.png)  
아래가 성능을 향상시킨 점수이다. 둘 다 과대적합을 잘 억제하고 있다.     

### 히스토그램 기반 그레이디언트 부스팅   
특성의 값들을 256개 구간으로 나눠 최적의 분할을 빠르게 찾는다.    
``` python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemle import HistGradientBoostingClassifer
hgb = HistGradientBoostingClassifer(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
```
#### 추가적인 라이브러리 
#### XGBoost
```python
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
```
#### LightGBM
```python
from lightgbm import LGBMClassifer
lgb = LGBMClassifer(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True)
```
